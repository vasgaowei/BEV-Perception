# Awesome Bird's Eye View Perception
This is a repository for Bird's Eye View Perception, including 3D object detection, segmentation, online-mapping and occupancy prediction.

## News
```
- 2023.05.09: An initial version of recent papers or projects.
- 2023.05.12: Adding paper for 3D object detection.
- 2023.05.14: Adding paper for BEV segmentation, HD-map construction, Occupancy prediction and motion planning.
```

## Contents

## Papers
- [Survey](#survey)
- [3D Object Detection](#3d-object-detection) 
  - [Radar Lidar](#radar-lidar)
  - [Radar Camera](#radar-camera)
  - [Lidar Camera](#lidar-camera)
  - [Lidar](#lidar)
  - [Monocular](#monocular)
  - [Multiple Camera](#multiple-camera)
- [BEV Segmentation](#bev-segmentation)
  - [Lidar Camera](#lidar-camera)
  - [Lidar](#lidar)
  - [Monocular](#monocular)
  - [Multiple Camera](#multiple-camera)
- [Tracking](#tracking)
- [Perception Prediction Planning](#perception-prediction-planning)
  - [Monocular](#monocular)
  - [Multiple Camera](#multiple-camera)
- [Mapping](#mapping)
  - [Lidar](#lidar)
  - [Lidar Camera](#lidar-camera)
  - [Monocular](#monocular)
  - [Multiple Camera](#multiple-camera)
- [LaneGraph](#lanegraph)
  - [Monocular](#monocular)
- [Locate](#locate)
- [Occupancy Prediction](#occupancy-prediction)
  - [Occupancy Challenge](#occupancy-challenge)
- [Challenge](#challenge)
- [Dataset](#dataset)
- [World Model](#world-model)
- [Other](#other)

### Survey
- Vision-Centric BEV Perception: A Survey (Arxiv 2022)[[Paper]](https://arxiv.org/abs/2208.02797) [[Github]](https://github.com/4DVLab/Vision-Centric-BEV-Perception)
- Delving into the Devils of Bird’s-eye-viewPerception: A Review, Evaluation and Recipe (Arxiv 2022) [[Paper]](https://arxiv.org/abs/2209.05324) [[Github]](https://github.com/OpenDriveLab/BEVPerception-Survey-Recipe)
### 3D Object Detection
#### Radar Lidar
- RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2211.06108.pdf)
- Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D DynamicObject Detection (CVPR 2023) [[paper]](https://arxiv.org/pdf/2306.01438.pdf) [[Github]](https://github.com/JessieW0806/Bi-LRFusion)
- MaskBEV: Joint Object Detection and Footprint Completion for Bird’s-eye View 3D Point Clouds (IORS 2023) [[Paper]](https://arxiv.org/pdf/2307.01864.pdf) [[Github]](https://github.com/norlab-ulaval/mask_bev)
- LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.00724.pdf)
- HGSFusion: Radar-Camera Fusion with Hybrid Generation and Synchronization for 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.11489) [[Github]](https://github.com/garfield-cpp/HGSFusion)
#### Radar Camera
- CRAFT: Camera-Radar 3D Object Detectionwith Spatio-Contextual Fusion Transformer (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2209.06535.pdf)
- RadSegNet: A Reliable Approach to Radar Camera Fusion (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2208.03849.pdf)
- Bridging the View Disparity of Radar and Camera Features for Multi-modal Fusion 3D Object Detection (IEEE TIV 2023) [[Paper]](https://arxiv.org/pdf/2208.12079.pdf)
- CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception (ICLRW 2023) [[Paper]](https://arxiv.org/pdf/2304.00670.pdf)
- RC-BEVFusion: A Plug-In Module for Radar-CameraBird’s Eye View Feature Fusion (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.15883.pdf)
- RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection (CVPR 2024) [[Paper]](https://arxiv.org/abs/2403.16440) [[Github]](https://github.com/VDIGPKU/RCBEVDet)
- UniBEVFusion: Unified Radar-Vision BEVFusion for 3D Object Detection (Arxiv 2024) [[paper]](https://arxiv.org/abs/2409.14751)
- SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2411.19860) [[Github]](https://github.com/phi-wol/sparc)
- RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential Decoder for 3D Object Detection (AAAI 2025 2024) [[Paper]](https://arxiv.org/pdf/2412.12799) [[Github]](https://github.com/liyih/RCTrans)
#### Lidar Camera
- Semantic bevfusion: rethink lidar-camera fusion in unified bird’s-eye view representation for 3d object detection (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2212.04675.pdf)
- Sparse Dense Fusion for 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.04179.pdf)
- EA-BEV: Edge-aware Bird' s-Eye-View Projector for 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.17895.pdf) [[Github]](https://github.com/hht1996ok/EA-BEV)
- MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection (CVPR 2023) [[paper]](https://arxiv.org/pdf/2209.03102.pdf) [[Github]](https://github.com/SxJyJay/MSMDFusion)
- FULLER: Unified Multi-modality Multi-task 3D Perception via Multi-level Gradient Calibration (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.16617.pdf)
- Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.07152.pdf)
- SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2309.07084.pdf) [[Github]](https://github.com/IranQin/SupFusion)
- 3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.03742.pdf)
- FUSIONVIT: HIERARCHICAL 3D OBJECT DETECTION VIA LIDAR-CAMERA VISION TRANSFORMER FUSION (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.03620.pdf)
- Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using transformers (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.14919.pdf)
- PVTransformer: Point-to-Voxel Transformer for Scalable 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.02811)
- Learned Multimodal Compression for Autonomous Driving (IEEE MMSP 2024) [[Paper]](https://arxiv.org/pdf/2408.08211)
- Co-Fix3D: Enhancing 3D Object Detection with Collaborative Refinement (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2408.07999)
- SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.05292)
- Timealign: A Multi-Modal Object Detection Method For Time Misalignment Fusing In Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.10033)
- Semantic-Supervised Spatial-Temporal Fusion for LiDAR-based 3D Object Detection (ICRA 2025) [[Paper]](https://arxiv.org/abs/2503.10579)
#### Lidar
- MGTANet: Encoding Sequential LiDAR Points Using Long Short-Term Motion-Guided Temporal Attention for 3D Object Detection (AAAI 2023)[[paper]](https://arxiv.org/pdf/2212.00442.pdf)[[Github]](https://github.com/HYjhkoh/MGTANet)
- PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.03982.pdf)
- V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.04409.pdf)
- SEED: A Simple and Effective 3D DETR in Point Clouds (ECCV 2024) [[Paper]](https://arxiv.org/pdf/2407.10749) [[Github]](https://github.com/happinesslz/SEED)
#### Monocular
- Learning  2D  to  3D  Lifting  for  Object  Detection  in  3Dfor  Autonomous  Vehicles (IROS 2019) [[Paper]](https://arxiv.org/abs/1904.08494) [[Project Page](https://www.nec-labs.com/research/media-analytics/projects/learning-2d-to-3d-lifting-for-object-detection-in-3d-for-autonomous-vehicles/)
- Orthographic Feature Transform for Monocular 3D Object Detection (BMVC 2019) [[Paper]](http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2019-BMVC-Orthographic-Feature-Transform.pdf) [[Github]](https://github.com/tom-roddick/oft)
- BEV-MODNet: Monocular Camera-based Bird's Eye View Moving Object Detection for Autonomous Driving (ITSC 2021) [[Paper]](https://arxiv.org/abs/2107.04937) [[Project Page]](https://sites.google.com/view/bev-modnet)
- Categorical Depth Distribution Network for Monocular 3D Object Detection (CVPR 2021) [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Reading_Categorical_Depth_Distribution_Network_for_Monocular_3D_Object_Detection_CVPR_2021_paper.pdf) [[Github]](https://github.com/TRAILab/CaDDN)
- PersDet: Monocular 3D Detection in Perspective Bird’s-Eye-View (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2208.09394.pdf)
- Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving (CVPR 2022) [[Paper]](https://arxiv.org/pdf/2205.14882.pdf)
- Monocular 3D Object Detection with Depth from Motion (ECCV 2022) [[paper]](https://arxiv.org/pdf/2207.12988.pdf)[[Github]](https://github.com/Tai-Wang/Depth-from-Motion)
- MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.09421v1.pdf) [[Github]](https://github.com/cskkxjk/MonoNeRD)
- S3-MonoDETR: Supervised Shape&Scale-perceptive Deformable Transformer for Monocular 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.00928.pdf) [[Github]](https://github.com/mikasa3lili/S3-MonoDETR)
- MonoGAE: Roadside Monocular 3D Object Detection with Ground-Aware Embeddings (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.00400.pdf)
- YOLO-BEV: Generating Bird's-Eye View in the Same Way as 2D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.17379.pdf)
- UniMODE: Unified Monocular 3D Object Detection (CVPR 2024) [[Paper]](https://arxiv.org/abs/2402.18573v1)
- Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving (Arxuv 2024) [[paper]](https://arxiv.org/abs/2403.02037) [[Github]](https://github.com/Owen-Liuyuxuan/visionfactory)
- UniMODE: Unified Monocular 3D Object Detection (CVPR 2024) [[Paper]](https://arxiv.org/abs/2402.18573)
- MonoDETRNext: Next-generation Accurate and Efficient Monocular 3D Object Detection Method (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.15176)
- MonoDGP: Monocular 3D Object Detection with Decoupled-Query and Geometry-Error Priors (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.19590)
#### Multiple Camera
- Object DGCNN: 3D Object Detection using Dynamic Graphs (NIPS 2021) [[Paper]](https://arxiv.org/pdf/2110.06923.pdf)[[Github]](https://github.com/WangYueFt/detr3d)
- BEVDet: High-Performance Multi-Camera 3D Object Detection in Bird-Eye-View (Arxiv 2022) [[Paper]](https://arxiv.org/abs/2112.11790) [[Github]](https://github.com/HuangJunJie2017/BEVDet)
- DETR3D：3D Object Detection from Multi-view Image via 3D-to-2D Queries (CORL 2021) [[Paper]](https://arxiv.org/abs/2110.06922) [[Github]](https://github.com/WangYueFt/detr3d)
- BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework (NeurIPS 2022) [[Paper]](https://arxiv.org/abs/2205.13790)[[Github]](https://github.com/ADLab-AutoDrive/BEVFusion)
- Unifying Voxel-based Representation withTransformer for 3D Object Detectio (NeurIPS 2022) [[paper]](https://arxiv.org/pdf/2206.00630.pdf)[[Github]](https://github.com/dvlab-research/UVTR)
- Polar Parametrization for Vision-based Surround-View 3D Detection (arxiv 2022) [[Paper]](https://arxiv.org/abs/2206.10965) [[Github]](https://github.com/hustvl/PolarDETR)
- SRCN3D: Sparse R-CNN 3D Surround-View Camera Object Detection and Tracking for Autonomous Driving  (Arxiv 2022) [[Paper]](https://arxiv.org/abs/2206.14451) [[Github]](https://github.com/synsin0/SRCN3D)
- BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection (Arxuv 2022) [[Paper]](https://arxiv.org/pdf/2203.17054.pdf) [[Github]](https://github.com/HuangJunJie2017/BEVDet)
- BEVStereo: Enhancing Depth Estimation in Multi-view 3D Object Detection with Dynamic Temporal Stere (Arxiv 2022) [[Paper]](https://arxiv.org/abs/2209.10248)[[Github]](https://github.com/Megvii-BaseDetection/BEVStereo)
- MV-FCOS3D++: Multi-View Camera-Only 4D Object Detection with Pretrained Monocular Backbones (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2207.12716.pdf) [[Github]](https://github.com/Tai-Wang/Depth-from-Motion)
- Focal-PETR: Embracing Foreground for Efficient Multi-Camera 3D Object （Arxiv 2022）[[Paper]](https://arxiv.org/pdf/2212.05505.pdf)
- DETR4D: Direct Multi-View 3D Object Detection with Sparse Attention (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2212.07849.pdf)
- Multi-Camera Calibration Free BEV Representation for 3D Object Detection (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2210.17252.pdf)
- SemanticBEVFusion: Rethink LiDAR-Camera Fusion in Unified Bird's-Eye View Representation for 3D Object Detectio (IROS 2023) [[Paper]](https://arxiv.org/pdf/2212.04675.pdf)
- BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2212.04675.pdf)
- STS: Surround-view Temporal Stereo for Multi-view 3D Detection (Arxiv 2022) [[Paper]](https://arxiv.org/abs/2208.10145)
- BEV-LGKD: A Unified LiDAR-Guided Knowledge Distillation Framework for BEV 3D Object Detection (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2212.00623.pdf)
- Multi-Camera Calibration Free BEV Representation for 3D Object Detection (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2210.17252.pdf)
- AutoAlign: Pixel-Instance Feature Aggregationfor Multi-Modal 3D Object Detection (IJCAI 2022) [[Paper]](https://www.ijcai.org/proceedings/2022/0116.pdf) 
- Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection (ACM MM 2022) [[paper]](https://github.com/zehuichen123/Graph-DETR3D)[[Github]](https://github.com/zehuichen123/Graph-DETR3D)
- ORA3D: Overlap Region Aware Multi-view 3D Object Detection (BMVC 2022) [[Paper]](https://arxiv.org/pdf/2207.00865.pdf) [[Project Page]](https://kuai-lab.github.io/bmvc2022ora3d/)
- AutoAlignV2: Deformable Feature Aggregation for DynamicMulti-Modal 3D Object Detection (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680616.pdf)[[Github]](https://github.com/zehuichen123/AutoAlignV2)
- CenterFormer: Center-based Transformer for 3D Object Detection (ECCV 2022) [[paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980487.pdf)[[Github]](https://github.com/TuSimple/centerformer)
- SpatialDETR: Robust Scalable Transformer-Based 3D Object Detection from Multi-View Camera Images with Global Cross-Sensor Attention (ECCV 2022) [[Paper]]([[https://markus-enzweiler.de/downloads/publications/ECCV2022-spatial_detr.pdf](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990226.pdf)](https://arxiv.org/pdf/2211.14710.pdf))[[Github]](https://github.com/cgtuebingen/SpatialDETR)
- Position Embedding Transformation for Multi-View 3D Object Detection (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870523.pdf) [[Github]](https://github.com/megvii-research/PETR)
- BEVDepth: Acquisition of Reliable Depth forMulti-view 3D Object Detection (AAAI 2023) [[Paper]](https://arxiv.org/abs/2206.10092) [[Github]](https://github.com/Megvii-BaseDetection/BEVDepth)
- PolarFormer: Multi-camera 3D Object Detectionwith Polar Transformers (AAAI 2023) [[Paper]](https://arxiv.org/abs/2206.15398)[[Github]](https://github.com/fudan-zvg/PolarFormer)
- A Simple Baseline for Multi-Camera 3D Object Detection (AAAI 2023) [[Paper]](https://arxiv.org/pdf/2208.10035.pdf)[[Github]](https://github.com/zhangyp15/SimMOD)
- Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2301.01283.pdf) [[Github]](https://github.com/junjie18/CMT)
- Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2211.10581.pdf) [[Github]](https://github.com/linxuewu/Sparse4D)
- BEVSimDet: Simulated Multi-modal Distillation in Bird's-Eye View for Multi-view 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.16818.pdf)[[Github]](https://github.com/ViTAE-Transformer/BEVSimDet)
- BEVStereo++: Accurate Depth Estimation in Multi-view 3D Object Detection via Dynamic Temporal Stereo (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.04185.pdf) 
- BSH-Det3D: Improving 3D Object Detection with BEV Shape Heatmap (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.02000.pdf) [[Github]](https://github.com/mystorm16/BSH-Det3D)
- DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object Detection and Tracking (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.16628.pdf) [[Github]](https://github.com/SmartBot-PJLab/DORT)
- Geometric-aware Pretraining for Vision-centric 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.03105.pdf) [[Github]](https://github.com/OpenDriveLab/BEVPerception-Survey-Recipe)
- Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D Perception (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.05970.pdf)
- OA-BEV: Bringing Object Awareness to Bird's-Eye-View Representation for Multi-Camera 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2301.05711.pdf) 
- Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2304.00967.pdf) [[Github]](https://github.com/sense-x/hop)
- VIMI: Vehicle-Infrastructure Multi-view Intermediate Fusion for Camera-based 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.10975.pdf)
- Object as Query: Equipping Any 2D Object Detector with 3D Detection Ability (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2301.02364.pdf)
- VoxelFormer: Bird’s-Eye-View Feature Generation based on Dual-view Attention for Multi-view 3D Object Detection (Arxiv 2023) [[Paper]](https://github.com/Lizhuoling/VoxelFormer-public) [[Github]](https://arxiv.org/pdf/2304.01054.pdf)
- TiG-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2212.13979.pdf) [[Github]](https://github.com/ADLab3Ds/TiG-BEV)
- CrossDTR:  Cross-view  and  Depth-guided  Transformersfor  3D  Object  Detection (ICRA 2023) [[Paper]](https://arxiv.org/pdf/2209.13507.pdf)[[Github]](https://github.com/sty61010/CrossDTR)
- SOLOFusion: Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection (ICLR 2023) [[paper]](https://arxiv.org/abs/2210.02443)[[Github]](https://github.com/Divadi/SOLOFusion)
- BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection (ICLR 2023) [[Paper]](https://openreview.net/pdf?id=-2zfgNS917)[[Github]](https://github.com/zehuichen123/BEVDistill)
- UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View (CVPR 2023)[[Paper]](https://arxiv.org/pdf/2303.15083.pdf)[[Github]](https://github.com/megvii-research/CVPR2023-UniDistill)
- Understanding the Robustness of 3D Object Detection with Bird's-Eye-View Representations in Autonomous Driving (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2303.17297.pdf) 
- Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2303.06880.pdf) [[Github]](https://github.com/PJLab-ADG/3DTrans)
- Aedet: Azimuth-invariant multi-view 3d object detection (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2211.12501.pdf) [[Github]](https://github.com/fcjian/AeDet) [[Project]](https://fcjian.github.io/aedet/)
- BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2303.08498.pdf) [[Github]](https://github.com/ADLab-AutoDrive/BEVHeight)
- CAPE: Camera View Position Embedding for Multi-View 3D Object Detection (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2303.10209.pdf) [[Github]](https://github.com/kaixinbear/CAPE)
-  FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2301.04467.pdf) [[Github]](https://github.com/robertwyq/frustum)
-  Sparse4D v2 Recurrent Temporal Fusion with Sparse Model （Arxiv 2023） [[Paper]](https://arxiv.org/pdf/2305.14018.pdf) [[Github]](https://github.com/linxuewu/Sparse4D)
-  DA-BEV : Depth Aware BEV Transformer for 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2302.13002.pdf)
-  BEV-IO: Enhancing Bird’s-Eye-View 3D Detectionwith Instance Occupancy (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.16829.pdf)
-  OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection (Arxiv) [[Paper]](https://arxiv.org/pdf/2306.01738.pdf)
-  SA-BEV: Generating Semantic-Aware Bird’s-Eye-View Feature for Multi-view 3D Object Detection (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2307.11477.pdf) [[Github]](https://github.com/mengtan00/SA-BEV)
-  Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2306.08528.pdf)
-  DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.12972.pdf)
-  Far3D: Expanding the Horizon for Surround-view 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.09616.pdf)
-  HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird’s Eye View (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2307.13510.pdf)
- Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2303.11926.pdf) [[Github]](https://github.com/exiawsh/StreamPETR)
-  3DPPE: 3D Point Positional Encoding for Multi-Camera 3D Object Detection Transformers (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2211.14710.pdf) [[Github]](https://github.com/drilistbox/3DPPE) [[Github]](https://github.com/FiveLu/stream3dppe)
-  FB-BEV: BEV Representation from Forward-Backward View Transformations (ICCV 2023) [[paper]](https://arxiv.org/pdf/2308.02236.pdf) [[Github]](https://github.com/NVlabs/FB-BEV)
-  QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.10515v1.pdf)
-  SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.09244v1.pdf) [[Github]](https://github.com/MCG-NJU/SparseBEV)
-  NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection (ICCV 2023) [[paper]](https://arxiv.org/pdf/2307.14620.pdf) [[Github]](https://github.com/facebookresearch/NeRF-Det)
-  DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DistillBEV_Boosting_Multi-Camera_3D_Object_Detection_with_Cross-Modal_Knowledge_Distillation_ICCV_2023_paper.html)
-  BEVHeight++: Toward Robust Visual Centric 3D Object Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.16179.pdf)
-  UniBEV: Multi-modal 3D Object Detection with Uniform BEV Encoders for Robustness against Missing Sensor Modalities (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.14516.pdf)
-  Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.14491.pdf)
-  Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection (ICCV 2023) [[Paper]](https://browse.arxiv.org/pdf/2310.01401.pdf) [[Github]](https://github.com/ymingxie/parq) [[Project]](https://ymingxie.github.io/parq/)
-  CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird's-Eye View Fusion (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.06008.pdf)
-  DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.05989.pdf)
-  TOWARDS GENERALIZABLE MULTI-CAMERA 3D OBJECT DETECTION VIA PERSPECTIVE DEBIASING (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.11346.pdf)
-  Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection (NeurIPS 2023) (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.15670.pdf) [[Github]](https://github.com/OpenDriveLab/Birds-eye-view-Perception)
-  M&M3D: Multi-Dataset Training and Efficient Network for Multi-view 3D Object (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.00986.pdf)
-  Sparse4D v3 Advancing End-to-End 3D Detection and Tracking (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.11722.pdf) [[Github]](https://github.com/linxuewu/Sparse4D)
-  BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.01696.pdf)
-  Towards Efficient 3D Object Detection in Bird’s-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach [[Paper]](https://arxiv.org/pdf/2312.00633.pdf)
-  Residual Graph Convolutional Network for Bird”s-Eye-View Semantic Segmentation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.04044.pdf)
-  Diffusion-Based Particle-DETR for BEV Perception (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.11578.pdf)
-  M-BEV: Masked BEV Perception for Robust Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.12144.pdf)
-  Explainable Multi-Camera 3D Object Detection with Transformer-Based Saliency Maps (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.14606.pdf)
-  Sparse Dense Fusion for 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.04179.pdf)
-  WidthFormer: Toward Efficient Transformer-based BEV View Transformation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2401.03836.pdf) [[Github]](https://github.com/ChenhongyiYang/WidthFormer)
-  UniVision: A Unified Framework for Vision-Centric 3D Perception (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.06994.pdf)
-  DA-BEV: Unsupervised Domain Adaptation for Bird's Eye View Perception (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.08687.pdf)
-  Towards Scenario Generalization for Vision-based Roadside 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.16110.pdf) [[Github]]()
-  CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow (CVPR 2024) [[Paper]](https://arxiv.org/abs/2403.08919)
-  GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.11848)
-  Lifting Multi-View Detection and Tracking to the Bird's Eye View (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.12573) [[Github]](https://github.com/tteepe/TrackTacular)
-  DuoSpaceNet: Leveraging Both Bird's-Eye-View and Perspective View Representations for 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.10577)
-  BEVSpread: Spread Voxel Pooling for Bird’s-Eye-View Representation in Vision-based Roadside 3D Object Detection (CVPR 2024) [[Paper]]() [[Github]](https://github.com/DaTongjie/BEVSpread)
-  OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection (ECCV 2024) [[Paper]](https://arxiv.org/pdf/2407.10753) [[Github]](https://github.com/AlmoonYsl/OPEN)
-  FSD-BEV: Foreground Self-Distillation for Multi-view 3D Object Detection (ECCV 2024) [[Paper]](https://arxiv.org/pdf/2407.10135)
-  PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird's-Eye-View (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.16200)
-  GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.01816)
-  Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression (ECCV 2024) [[Paper]](https://arxiv.org/abs/2409.00633) [[Github]](https://github.com/DYZhang09/ToC3D)
-  MambaBEV: An efficient 3D detection model with Mamba2 (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.12673)
-  ROA-BEV: 2D Region-Oriented Attention for BEV-based 3D Object (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.10298)
-  Test-time Correction with Human Feedback: An Online 3D Detection System via Visual Prompting (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2412.07768)
-  HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for Multi-View 3D Object Detection (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.18884)
-  TiGDistill-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning Distillation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.20911) [[Github]](https://github.com/Public-BOTs/TiGDistill-BEV)
-  DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2503.11122)
### BEV Segmentation
#### Lidar Camera
- PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images (Axxiv 2023) [[Paper]](https://arxiv.org/pdf/2206.01256.pdf) [[Github]](https://github.com/megvii-research/PETR)
- X-Align: Cross-Modal Cross-View Alignment for Bird’s-Eye-View Segmentation (WACV 2023) [[Paper]](https://openaccess.thecvf.com/content/WACV2023/papers/Borse_X-Align_Cross-Modal_Cross-View_Alignment_for_Birds-Eye-View_Segmentation_WACV_2023_paper.pdf) 
- BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation (ICRA 2023) [[Paper]](https://arxiv.org/pdf/2205.13542.pdf) [[Github]](https://github.com/mit-han-lab/bevfusion) [[Project]](https://bevfusion.mit.edu/)
  UniM2AE: Multi-modal Masked Autoencoders with Unified 3D Representation for 3D Perception in Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.10421.pdf)
- BEVFusion4D: Learning LiDAR-Camera Fusion Under Bird's-Eye-View via Cross-Modality Guidance and Temporal Aggregation (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.17099.pdf)
- Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.11325.pdf) 
- LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2304.11379.pdf) [[Github]](https://github.com/songw-zju/LiDAR2Map)
- BEV-Guided Multi-Modality Fusion for Driving Perception (CVPR 2023) [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.pdf) [[Github]](https://yunzeman.github.io/BEVGuide)
- FUSIONFORMER: A MULTI-SENSORY FUSION IN BIRD’S-EYE-VIEW AND TEMPORAL CONSISTENT TRANSFORMER FOR 3D OBJECTION (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.05257v1.pdf)
- UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.07732.pdf) [[Github]](https://github.com/Haiyang-W/UniTR)
- BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Bird’s Eye View Map Construction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.11119.pdf)
- BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.11761)
- OE-BevSeg: An Object Informed and Environment Aware Multimodal Framework for Bird's-eye-view Vehicle Semantic Segmentation (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.13137)
- BEVPose: Unveiling Scene Semantics through Pose-Guided Multi-Modal BEV Alignment (IROS 2024) [[Paper]](https://arxiv.org/pdf/2410.20969) [[Project]](https://m80hz.github.io/bevpose/)
- PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR Semantic Segmentation (AAAI 2025) [[paper]](https://arxiv.org/abs/2412.14821) [[Paper]](https://github.com/skyshoumeng/PC-BEV)
#### Lidar
- LidarMultiNet: Unifying LiDAR Semantic Segmentation, 3D Object Detection, and Panoptic Segmentation in a Single Multi-task Network (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2206.11428.pdf)
- SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.13323.pdf)
- BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds (3DV 2023) [[Paper]](https://arxiv.org/pdf/2310.17281.pdf) [[Github]](https://github.com/valeoai/BEVContrast)
#### Monocular
- Learning to Look around Objects for Top-View Representations of Outdoor Scenes (ECCV 2018) [[paper]](https://arxiv.org/pdf/1803.10870.pdf)
- A Parametric Top-View Representation of Complex Road Scenes (CVPR 2019) [[Paper]](https://arxiv.org/pdf/1812.06152.pdf)
- Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks (ICRA 2019 IEEE RA-L 2019) [[Paper]](https://arxiv.org/pdf/1804.02176.pdf) [[Github]](https://github.com/Chenyang-Lu/mono-semantic-occupancy)
- Short-Term Prediction and Multi-Camera Fusion on Semantic Grids (ICCVW 2019) [[paper]](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CVRSUAD/Hoyer_Short-Term_Prediction_and_Multi-Camera_Fusion_on_Semantic_Grids_ICCVW_2019_paper.pdf)
- Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks (CVPR 2020) [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Roddick_Predicting_Semantic_Map_Representations_From_Images_Using_Pyramid_Occupancy_Networks_CVPR_2020_paper.pdf) [[Github]](https://github.com/tom-roddick/mono-semantic-maps)
- MonoLayout : Amodal scene layout from a single image (WACV 2020) [[Paper]](https://openaccess.thecvf.com/content_WACV_2020/papers/Mani_MonoLayout_Amodal_scene_layout_from_a_single_image_WACV_2020_paper.pdf) [[Github]](https://github.com/manila95/monolayout)
- Bird’s Eye View Segmentation Using Lifted2D Semantic Features (BMVC 2021) [[Paper]](https://www.bmvc2021-virtualconference.com/assets/papers/0772.pdf) 
- Enabling Spatio-temporal aggregation in Birds-Eye-View Vehicle Estimation (ICRA 2021) [[Paper]](https://cvssp.org/Personal/OscarMendez/papers/pdf/SahaICRA2021.pdf) [[mp4]](https://cvssp.org/Personal/OscarMendez/videos/SahaICRA2021.mp4)
- Projecting Your View Attentively: Monocular Road Scene Layout Estimation viaCross-view Transformation (CVPR 2021) [[Paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf) [[Github]](https://github.com/JonDoe-297/cross-view)
- ViT BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation (IEEE IJCNN 2022) [[paper]](https://arxiv.org/pdf/2205.15667.pdf)
- Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images (IEEE RA-L 2022) [[Paper]](https://arxiv.org/pdf/2108.03227.pdf) [[Github]](https://github.com/robot-learning-freiburg/PanopticBEV) [[Project]](http://panoptic-bev.cs.uni-freiburg.de/)
- Understanding Bird's-Eye View of Road Semantics using an Onboard Camera (ICRA 2022) [[Paper]](https://arxiv.org/pdf/2012.03040.pdf) [[Github]](https://github.com/ybarancan/BEV_feat_stitch)
- “The Pedestrian next to the Lamppost”Adaptive Object Graphs for Better Instantaneous Mapping (CVPR 2022) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Saha_The_Pedestrian_Next_to_the_Lamppost_Adaptive_Object_Graphs_for_CVPR_2022_paper.pdf)
- Weakly But Deeply Supervised Occlusion-Reasoned Parametric Road Layouts (CVPR 2022) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Weakly_but_Deeply_Supervised_Occlusion-Reasoned_Parametric_Road_Layouts_CVPR_2022_paper.pdf)
- Translating Images into Maps (ICRA 2022) [[Paper]](https://arxiv.org/pdf/2110.00966.pdf) [[Github]](https://github.com/avishkarsaha/translating-images-into-maps)
- GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610390.pdf)
- SBEVNet: End-to-End Deep Stereo Layout Estimation (WACV 2022) [[Paper]](https://openaccess.thecvf.com/content/WACV2022/papers/Gupta_SBEVNet_End-to-End_Deep_Stereo_Layout_Estimation_WACV_2022_paper.pdf)
- BEVSegFormer: Bird’s Eye View Semantic Segmentation From ArbitraryCamera Rigs (WACV 2023) [[Paper]](https://openaccess.thecvf.com/content/WACV2023/papers/Peng_BEVSegFormer_Birds_Eye_View_Semantic_Segmentation_From_Arbitrary_Camera_Rigs_WACV_2023_paper.pdf)
- DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.08333.pdf) [[Github]](https://github.com/JiayuZou2020/DiffBEV)
- HFT: Lifting Perspective Representations via Hybrid Feature Transformation (ICRA 2023) [[Paper]](https://arxiv.org/pdf/2204.05068.pdf) [[Github]](https://github.com/JiayuZou2020/HFT)
- SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2302.04233.pdf)
- Calibration-free BEV Representation for Infrastructure Perception (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.03583.pdf)
- Semi-Supervised Learning for Visual Bird’s Eye View Semantic Segmentation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.14525.pdf)
- DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEVPerception (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2305.03724.pdf) [[github]](https://github.com/YunzeMan/DualCross) [[Project]](https://yunzeman.github.io/DualCross/)
- CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.02815.pdf)
- SeaBird: Segmentation in Bird’s View with Dice Loss Improves Monocular 3D Detection of Large Objects (CVPR 2024) [[Paper]](https://arxiv.org/abs/2403.20318) [[Github]](https://github.com/abhi1kumar/SeaBird)
- DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird's Eye View Segmentation with Occlusion Reasoning (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2404.06352) [[Github]](https://streamable.com/ge4v51)
- Improved Single Camera BEV Perception Using Multi-Camera Training (ITSC 2024) [[Paper]](https://arxiv.org/abs/2409.02676)
- Focus on BEV: Self-calibrated Cycle View Transformation for Monocular Birds-Eye-View Segmentation (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2410.15932)
- Geo-ConvGRU: Geographically Masked Convolutional Gated Recurrent Unit for Bird-Eye View Segmentation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.20171)
#### Multiple Camera
- A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird’s Eye View （IEEE ITSC 2020）[[Paper]](https://arxiv.org/pdf/2005.04078.pdf) [[Github]](https://github.com/ika-rwth-aachen/Cam2BEV)
- Cross-view Semantic Segmentation for Sensing Surroundings (IROS 2020 IEEE RA-L 2020) [[Paper]](https://arxiv.org/pdf/1906.03560.pdf) [[Github]](https://github.com/pbw-Berwin/View-Parsing-Network) [[Project]](https://decisionforce.github.io/VPN/)
- Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D (ECCV 2020) [[Paper]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590188.pdf) [[Github]](https://github.com/nv-tlabs/lift-splat-shoot) [[Project]](https://nv-tlabs.github.io/lift-splat-shoot/)
- Cross-view Transformers for real-time Map-view Semantic Segmentation (CVPR 2022) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf) [[Github]](https://github.com/bradyz/cross_view_transformers)
- Scene Representation in Bird’s-Eye View from Surrounding Cameras withTransformers (CVPRW 2022) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Zhao_Scene_Representation_in_Birds-Eye_View_From_Surrounding_Cameras_With_Transformers_CVPRW_2022_paper.pdf)
- M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2204.05088.pdf) [[Project]](https://nvlabs.github.io/M2BEV/)
- BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2205.09743.pdf) [[Github]](https://github.com/zhangyp15/BEVerse)
- Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2206.04584.pdf) [[Github]](https://github.com/hustvl/GKT)
- A Simple Baseline for BEV Perception Without LiDAR (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2206.07959.pdf) [[Github]](https://github.com/aharley/simple_bev) [[Project Page]](https://simple-bev.github.io/)
- UniFusion: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2207.08536.pdf) [[Github](https://github.com/cfzd/UniFusion)
- LaRa: Latents and Rays for Multi-CameraBird’s-Eye-View Semantic Segmentation (CORL 2022) [[Paper]](https://proceedings.mlr.press/v205/bartoccioni23a/bartoccioni23a.pdf)) [[Github]](https://github.com/valeoai/LaRa)
- CoBEVT: Cooperative Bird’s Eye View Semantic Segmentation with Sparse Transformers (CORL 2022) [[Paper]](https://arxiv.org/pdf/2207.02202.pdf) [[Github]](https://github.com/DerrickXuNu/CoBEVT)
- Vision-based Uneven BEV Representation Learningwith Polar Rasterization and Surface Estimation (CORL 2022) [[Paper]](https://arxiv.org/pdf/2207.01878.pdf) [[Github]](https://github.com/SuperZ-Liu/PolarBEV)
- BEVFormer: a Cutting-edge Baseline for Camera-based Detection (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690001.pdf) [[Github]](https://github.com/fundamentalvision/BEVFormer)
- JPerceiver: Joint Perception Network for Depth, Pose and Layout Estimation in Driving Scenes (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980692.pdf) [[Github]](https://github.com/sunnyHelen/JPerceiver)
- Learning Ego 3D Representation as Ray Tracing (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860126.pdf) [[Github]](https://github.com/fudan-zvg/Ego3RT)
- Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception (NIPS 2022 Workshop) [[Paper]](https://arxiv.org/pdf/2301.07870.pdf) or [[Paper]](https://arxiv.org/pdf/2301.12511.pdf) [[Github]](https://github.com/Sense-GVT/Fast-BEV)
- Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2301.12511.pdf) [[Github]](https://github.com/Sense-GVT/Fast-BEV)
- BEVFormer v2: Adapting Modern Image Backbones toBird’s-Eye-View Recognition via Perspective Supervision (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2211.10439.pdf)
- MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models (CVPR 2023) [[Paper]](/https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_MapPrior_Birds-Eye_View_Map_Layout_Estimation_with_Generative_Models_ICCV_2023_paper.pdf)
- Bi-Mapper: Holistic BEV Semantic Mapping for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2305.04205.pdf) [[Github]](https://github.com/lynn-yu/Bi-Mapper)
- MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2211.10593.pdf) [[Github]](https://github.com/ZRandomize/MatrixVT)
- MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation (ICCV 2023) [[paper]](https://arxiv.org/pdf/2304.09801.pdf) [[Github]](https://github.com/ChongjianGE/MetaBEV) [[Project]](https://chongjiange.github.io/metabev.html)
- One Training for Multiple Deployments: Polar-based Adaptive BEV Perception for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.00525.pdf)
- RoboBEV: Towards Robust Bird's Eye View Perception under Corruptions (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2304.06719.pdf) [[Github]](https://github.com/Daniel-xsy/RoboBEV) [[Project]](https://daniel-xsy.github.io/robobev/)
- X-Align++: cross-modal cross-view alignment for Bird's-eye-view segmentation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2306.03810.pdf)
- PowerBEV: A Powerful Yet Lightweight Framework forInstance Prediction in Bird’s-Eye View (Axriv 2023) [[paper]](https://arxiv.org/pdf/2306.10761.pdf)
- Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird’s-Eye View (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2307.04106.pdf)
- Towards Viewpoint Robustness in Bird’s Eye View Segmentation (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2309.05192.pdf) [[Project]](https://nvlabs.github.io/viewpoint-robustness/)
- PowerBEV: A Powerful Yet Lightweight Framework for Instance Prediction in Bird’s-Eye View (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2306.10761.pdf)
- PointBeV: A Sparse Approach to BeV Predictions (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.00703.pdf) [[Github]](https://github.com/valeoai/PointBeV)
- DualBEV: CNN is All You Need in View Transformation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.05402)
- MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.08760)
- HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2404.02517) [[Github]](https://github.com/VDIGPKU/HENet)
- Improving Bird's Eye View Semantic Segmentation by Task Decomposition (CVPR 2024) [[Paper]](https://arxiv.org/abs/2404.01925) [[Github]](https://github.com/happytianhao/TaDe)
- SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation (CVPR 2024) [[Paper]](https://arxiv.org/abs/2404.02638) [[Github]](https://github.com/yejy53/SG-BEV)
- RoadBEV: Road Surface Reconstruction in Bird's Eye View (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2404.06605) [[Github]](https://github.com/ztsrxh/RoadBEV)
- TempBEV: Improving Learned BEV Encoders with Combined Image and BEV Space Temporal Aggregation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2404.11803)
- DiffMap: Enhancing Map Segmentation with Map Prior Using Diffusion Model (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.02008)
- Bird's-Eye View to Street-View: A Survey (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.08961)
- LetsMap: Unsupervised Representation Learning for Semantic BEV Mapping (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.18852)
- Navigation Instruction Generation with BEV Perception and Large Language Models (ECCV 2024) [[paper]](https://arxiv.org/abs/2407.15087) [[Github]](https://github.com/FanScy/BEVInstructor)
- GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV Segmentation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.14108)
- MaskBEV: Towards A Unified Framework for BEV Detection and Map Segmentation (ACM MM 2024) [[paper]](https://arxiv.org/abs/2408.09122)
- Robust Bird’s Eye View Segmentation by Adapting DINOv2 (ECCV 2024 Workshop) [[Paper]](https://arxiv.org/pdf/2409.10228)
- Unveiling the Black Box: Independent Functional Module Evaluation for Bird’s-Eye-View Perception Model (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.11969)
- RopeBEV: A Multi-Camera Roadside Perception Network in Bird's-Eye-View (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.11706)
- OneBEV: Using One Panoramic Image for Bird's-Eye-View Semantic Mapping (ACCV 2024) [[Paper]](https://arxiv.org/abs/2409.13912) [[Github]](https://github.com/JialeWei/OneBEV)
- ROAD-Waymo: Action Awareness at Scale for Autonomous Driving (NeurIPS 2024) [[Paper]](https://arxiv.org/abs/2411.01618) [[Github]](https://github.com/Z1zyw/VQ-Map)
- Fast and Efficient Transformer-based Method for Bird’s Eye View Instance Prediction (IEEE ITSC 2024) [[Paper]](https://arxiv.org/pdf/2411.06851) [[Github]](https://github.com/miguelag99/Efficient-Instance-Prediction)
- Epipolar Attention Field Transformers for Bird's Eye View Semantic Segmentation (WACV 2025) [[paper]](https://arxiv.org/abs/2412.01595)
- Revisiting Birds Eye View Perception Models with Frozen Foundation Models: DINOv2 and Metric3Dv2 (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2501.08118)
- SegLocNet: Multimodal Localization Network for Autonomous Driving via Bird's-Eye-View Segmentation (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.20077)
- BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2502.19694)
- Dur360BEV: A Real-world 360-degree Single Camera Dataset and Benchmark for Bird-Eye View Mapping in Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.00675)
- TS-CGNet: Temporal-Spatial Fusion Meets Centerline-Guided Diffusion for BEV Mapping (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.02578) [[Github]](https://github.com/krabs-H/TS-CGNet)
- BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation (Arxiv 2025) [[Paper[[(BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation)
- HierDAMap: Towards Universal Domain Adaptive BEV Mapping via Hierarchical Perspective Priors (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.06821)
- MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations (ICLR 2025) [[Paper]](https://arxiv.org/abs/2503.13858)
### Perception Prediction Planning
#### Monocular
- Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning (WACV 2021) [[Paper]](https://openaccess.thecvf.com/content/WACV2021/papers/Loukkal_Driving_Among_Flatmobiles_Bird-Eye-View_Occupancy_Grids_From_a_Monocular_Camera_WACV_2021_paper.pdf)
- HOPE: Hierarchical Spatial-temporal Network for Occupancy Flow Prediction (CVPRW 2022) [[paper]](https://arxiv.org/pdf/2206.10118.pdf)
#### Multiple Camera
- FIERY: Future Instance Prediction in Bird’s-Eye View from Surround Monocular Cameras (ICCV 2021) [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf) [[Github]](https://github.com/wayveai/fiery) [[Project]](https://anthonyhu.github.io/fiery)
- NEAT: Neural Attention Fields for End-to-End Autonomous Driving (ICCV 2021) [[Paper]](https://arxiv.org/pdf/2109.04456.pdf) [[Github]](https://github.com/autonomousvision/neat)
- ST-P3: End-to-end Vision-based AutonomousDriving via Spatial-Temporal Feature Learning (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980522.pdf) [[Github]](https://github.com/OpenPerceptionX/ST-P3)
- StretchBEV: Stretching Future InstancePrediction Spatially and Temporally (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980436.pdf) [[Github]](https://github.com/kaanakan/stretchbev) [[Projet]](https://kuis-ai.github.io/stretchbev/)
- TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2303.09998.pdf) [[Github]](https://github.com/MediaBrain-SJTU/TBP-Former)
- Planning-oriented Autonomous Driving (CVPR 2023, Occupancy Prediction) [[paper]](https://arxiv.org/pdf/2212.10156.pdf) [[Github]](https://github.com/OpenDriveLab/UniAD) [[Project]](https://opendrivelab.github.io/UniAD/)
- Think Twice before Driving:Towards Scalable Decoders for End-to-End Autonomous Driving (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2305.06242.pdf) [[Github]](https://github.com/OpenDriveLab/ThinkTwice)
- ReasonNet: End-to-End Driving with Temporal and Global Reasoning (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2305.10507.pdf)
- LiDAR-BEVMTN: Real-Time LiDAR Bird’s-Eye View Multi-Task Perception Network for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2307.08850.pdf)
- FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.01006.pdf)
- VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.13243) [[Project]](https://hgao-cv.github.io/VADv2/)
- SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2404.06892)
- SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2405.19620) [[Github]](https://github.com/swc-17/SparseDrive)
- DUALAD: Disentangling the Dynamic and Static World for End-to-End Driving (CVPR 2024) [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Doll_DualAD_Disentangling_the_Dynamic_and_Static_World_for_End-to-End_Driving_CVPR_2024_paper.pdf)
- Solving Motion Planning Tasks with a Scalable Generative Model (ECCV 2024) [[Paper]](/https://arxiv.org/pdf/2407.02797) [[Github]](https://github.com/HorizonRobotics/GUMP/)
### Mapping
#### Lidar
- Hierarchical Recurrent Attention Networks for Structured Online Map (CVPR 2018) [[Paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.pdf)
#### Lidar Camera
- End-to-End Deep Structured Models for Drawing Crosswalks (ECCV 2018) [[Paper]](https://arxiv.org/pdf/2012.11585.pdf) 
- Probabilistic Semantic Mapping for Urban Autonomous Driving Applications (IROS 2020) [[Paper]](http://ras.papercept.net/images/temp/IROS/files/2186.pdf) [[Github]](https://github.com/MediaBrain-SJTU/TBP-Former)
- Convolutional Recurrent Network for Road Boundary Extraction (CVPR 2022) [[Paper]](https://nhoma.github.io/papers/road_cvpr19.pdf)
- Lane Graph Estimation for Scene Understanding in Urban Driving (IEEE RAL 2021) [[Paper]](https://arxiv.org/pdf/2105.00195.pdf)
- M^2-3DLaneNet: Multi-Modal 3D Lane Detection (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2209.05996.pdf) [[Github]](https://github.com/JMoonr/mmlane)
- HDMapNet: An Online HD Map Construction and Evaluation Framework (ICRA 2022) [[paper]](https://arxiv.org/pdf/2107.06307.pdf) [[Github]](https://github.com/Tsinghua-MARS-Lab/HDMapNet) [[Project]](https://tsinghua-mars-lab.github.io/HDMapNet/)
- SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2211.15656.pdf) [[Github]](https://github.com/haomo-ai/SuperFusion)
- VMA: Divide-and-Conquer Vectorized MapAnnotation System for Large-Scale Driving Scene (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.09807.pdf)
- THMA: Tencent HD Map AI System for Creating HD Map Annotations (AAAI 2023) [[paper]](https://arxiv.org/pdf/2212.11123.pdf)
#### Monocular
- RoadTracer: Automatic Extraction of Road Networks from Aerial Images (CVPR 2018) [[Paper]](https://arxiv.org/pdf/1802.03680.pdf) [[Github]](https://github.com/mitroadmaps/roadtracer)
- DAGMapper: Learning to Map by Discovering Lane Topology (ICCV 2019) [[paper]](https://arxiv.org/pdf/2012.12377.pdf) 
- End-to-end Lane Detection through Differentiable Least-Squares Fitting (ICCVW 2019) [[paper]](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CVRSUAD/Van_Gansbeke_End-to-end_Lane_Detection_through_Differentiable_Least-Squares_Fitting_ICCVW_2019_paper.pdf)
- VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction (CVPR 2020) [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_VecRoad_Point-Based_Iterative_Graph_Exploration_for_Road_Graphs_Extraction_CVPR_2020_paper.pdf) [[Github]](https://github.com/tansor/VecRoad) [[Project]](https://mmcheng.net/vecroad/)
- Sat2Graph: Road Graph Extraction through Graph-Tensor Encoding (ECCV 2020) [[paper]](https://arxiv.org/pdf/2007.09547.pdf) [[Github]](https://github.com/songtaohe/Sat2Graph)
- iCurb: Imitation Learning-based Detection of Road Curbs using Aerial Images for Autonomous Driving (ICRA 2021 IEEE RA-L) [[paper]](https://arxiv.org/pdf/2103.17118.pdf) [[Github]](https://github.com/TonyXuQAQ/Topo-boundary/tree/master/graph_based_baselines/iCurb) [[Project]](https://tonyxuqaq.github.io/projects/iCurb/)
- HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps (CVPR 2021) [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Mi_HDMapGen_A_Hierarchical_Graph_Generative_Model_of_High_Definition_Maps_CVPR_2021_paper.pdf)
- Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images (ICCV 2021) [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf) [[Github]](https://github.com/ybarancan/STSU)
- RNGDet: Road Network Graph Detection by Transformer in Aerial Images (IEEE TGRS 2022) [[[Paper]](https://arxiv.org/pdf/2202.07824.pdf) [[Project]](https://tonyxuqaq.github.io/projects/RNGDet/)
- RNGDet++: Road Network Graph Detection by Transformer with Instance Segmentation and Multi-scale Features Enhancement (IEEE RA-L 2022) [[Paper]](https://tonyxuqaq.github.io/assets/pdf/2022_RAL_RNGDetPlusPlus.pdf) [[Github]](https://github.com/TonyXuQAQ/RNGDetPlusPlus) [[Project]](https://github.com/TonyXuQAQ/RNGDetPlusPlus)
- SPIN Road Mapper: Extracting Roads from Aerial Images via Spatial and Interaction Space Graph Reasoning for Autonomous Driving (ICRA 2022) [[paper]](https://arxiv.org/pdf/2109.07701.pdf) [[Github]](https://github.com/wgcban/SPIN_RoadMapper)
- Laneformer: Object-aware Row-Column Transformers for Lane Detection (AAAI 2022) [[Paper]](https://arxiv.org/pdf/2203.09830.pdf)
- Lane-Level Street Map Extraction from Aerial Imagery (WACV 2022) [[Paper]](https://openaccess.thecvf.com/content/WACV2022/papers/He_Lane-Level_Street_Map_Extraction_From_Aerial_Imagery_WACV_2022_paper.pdf) [[Github]](https://github.com/songtaohe/LaneExtraction)
- Reconstruct from Top View: A 3D Lane Detection Approach based on GeometryStructure Prior (CVPRW 2022) [[paper]](https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Li_Reconstruct_From_Top_View_A_3D_Lane_Detection_Approach_Based_CVPRW_2022_paper.pdf)
- PolyWorld: Polygonal Building Extraction with Graph Neural Networks in Satellite Images (CVPR 2022) [[Paper]](https://arxiv.org/pdf/2111.15491.pdf) [[Github]](https://github.com/zorzi-s/PolyWorldPretrainedNetwork)
- Topology Preserving Local Road Network Estimation from Single Onboard Camera Image (CVPR 2022) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Can_Topology_Preserving_Local_Road_Network_Estimation_From_Single_Onboard_Camera_CVPR_2022_paper.pdf) [[Github]](https://github.com/ybarancan/TopologicalLaneGraph)
- TD-Road: Top-Down Road Network Extraction with Holistic Graph Construction (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690553.pdf)
- CLiNet: Joint Detection of Road Network Centerlines in 2D and 3D (IEEE IVS 2023) [[Paper]](https://arxiv.org/pdf/2302.02259.pdf) 
- Polygonizer: An auto-regressive building delineator (ICLRW 2023) [[Paper]](https://arxiv.org/pdf/2304.04048.pdf)
- CurveFormer:  3D  Lane  Detection  by  Curve  Propagation  with  CurveQueries  and  Attention (ICRA 2023) [[Paper]](https://arxiv.org/pdf/2209.07989.pdf)
- Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection (CVPR 2023) [[paper]](https://arxiv.org/pdf/2301.02371.pdf) [[Github]](https://github.com/tusen-ai/Anchor3DLane)
- Learning and Aggregating Lane Graphs for Urban Automated Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2302.06175.pdf)
- Online Lane Graph Extraction from Onboard Video (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2304.00930.pdf) [[Github]](https://github.com/hustvl/LaneGAP)
- Video Killed the HD-Map: Predicting Driving BehaviorDirectly From Drone Images (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.11856.pdf)
- Prior Based Online Lane Graph Extraction from Single Onboard Camera Image (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.13344.pdf)
- Online Monocular Lane Mapping Using Catmull-Rom Spline (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.11653.pdf) [[Github]](https://github.com/HKUST-Aerial-Robotics/MonoLaneMapping)
- Improving Online Lane Graph Extraction by Object-Lane Clustering (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2307.10947.pdf)
- LATR: 3D Lane Detection from Monocular Images with Transformer (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.04583v1.pdf) [[Github]](https://github.com/JMoonr/LATR)
- Patched Line Segment Learning for Vector Road Mapping (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.02923.pdf)
- Sparse Point Guided 3D Lane Detection (ICCV 2023) [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Sparse_Point_Guided_3D_Lane_Detection_ICCV_2023_paper.pdf) [[Github]](https://github.com/YaoChengTang/Sparse-Point-Guided-3D-Lane-Detection)
- Recursive Video Lane Detection (ICCV 2023) [[Paper]](https://browse.arxiv.org/pdf/2308.11106.pdf) [[Github]](https://github.com/dongkwonjin/RVLD)
- LATR: 3D Lane Detection from Monocular Images with Transformer (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.04583.pdf) [[Github]](https://github.com/JMoonr/LATR)
- Occlusion-Aware 2D and 3D Centerline Detection for Urban Driving via Automatic Label Generation (ARXIV 2023) [[PAPER]](https://arxiv.org/pdf/2311.02044.pdf)
- BUILDING LANE-LEVEL MAPS FROM AERIAL IMAGES (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.13449.pdf)
- LaneCPP: Continuous 3D Lane Detection using Physical Priors (CVPR 2024) [[Paper]](https://arxiv.org/pdf/2406.08381)
- DeepAerialMapper: Deep Learning-based Semi-automatic HD Map Creation for Highly Automated Vehicles (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.00769) [[Github]]()
#### Multiple Camera
- PersFormer: a New Baseline for 3D Laneline Detection (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980539.pdf) [[Github]](https://github.com/OpenDriveLab/PersFormer_3DLane)
- Continuity-preserving Path-wise Modeling for Online Lane Graph Construction (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.08815.pdf) [[Github]](https://github.com/hustvl/LaneGAP)
- VAD: Vectorized Scene Representation for Efficient Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.12077.pdf) [[Github]](https://github.com/hustvl/VAD)
- InstaGraM: Instance-level Graph Modelingfor Vectorized HD Map Learning (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2301.04470.pdf)
- VectorMapNet: End-to-end Vectorized HD Map Learning (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2206.08920.pdf) [[Github]](https://github.com/Mrmoore98/VectorMapNet_code) [[Project]](https://tsinghua-mars-lab.github.io/vectormapnet/)
- Road Genome: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.10440.pdf) [[Github]](https://github.com/OpenDriveLab/OpenLane-V2)
- Topology Reasoning for Driving Scenes (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2304.05277.pdf) [[Github]](https://github.com/OpenDriveLab/TopoNet)
- MV-Map: Offboard HD-Map Generation with Multi-view Consistency （Arxiv 2023） [[paper]](https://arxiv.org/pdf/2305.08851.pdf) [[Github]](https://github.com/ZiYang-xie/MV-Map)
- CenterLineDet: Road Lane CenterLine Graph Detection With Vehicle-Mounted Sensors by Transformer for High-definition Map Creation (ICRA 2023) [[paper]](https://arxiv.org/pdf/2209.07734.pdf) [[Github]](https://github.com/TonyXuQAQ/CenterLineDet)
- Structured Modeling and Learning for Online Vectorized HD Map Construction (ICLR 2023) [[paper]](https://arxiv.org/pdf/2208.14437.pdf) [[Github]](https://github.com/hustvl/MapTR)
- Neural Map Prior for Autonomous Driving (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2304.08481.pdf)
- An Efficient Transformer for Simultaneous Learning of BEV and LaneRepresentations in 3D Lane Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2306.04927.pdf)
- TopoMask: Instance-Mask-Based Formulation for the Road Topology Problemvia Transformer-Based Architecture (Arxiv 2023) [[apper]](https://arxiv.org/pdf/2306.05419.pdf)
- PolyDiffuse: Polygonal Shape Reconstruction viaGuided Set Diffusion Models (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2306.01461.pdf) [[Github]](https://github.com/woodfrog/poly-diffuse) [[Project]](https://poly-diffuse.github.io/)
- Online Map Vectorization for Autonomous Driving: A Rasterization Perspective (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2306.10502.pdf)
- NeMO: Neural Map Growing System forSpatiotemporal Fusion in Bird’s-Eye-Viewand BDD-Map Benchmark (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2306.04540.pdf)
- MachMap: End-to-End Vectorized Solution for Compact HD-Map Construction (CVPR 2023 Workshop) [[Paper]](https://arxiv.org/pdf/2306.10301.pdf)
- Lane Graph as Path: Continuity-preserving Path-wise Modelingfor Online Lane Graph Construction (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.08815.pdf)
- End-to-End Vectorized HD-map Construction with Piecewise B ́ezier Curve (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2306.09700.pdf) [[Github]](https://github.com/er-muyue/BeMapNet)
- GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.09472.pdf)
- MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.05736.pdf)
- LATR: 3D Lane Detection from Monocular Images with Transformer (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.04583.pdf)
- INSIGHTMAPPER: A CLOSER LOOK AT INNER-INSTANCE INFORMATION FOR VECTORIZED HIGH-DEFINITION MAPPING (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.08543.pdf) [[Project]](https://tonyxuqaq.github.io/InsightMapper/) [[Github]](https://github.com/TonyXuQAQ/InsightMapper/tree/main)
- HD Map Generation from Noisy Multi-Route Vehicle Fleet Data on Highways with Expectation Maximization (Arxiv 2023) [[Paper]](https://browse.arxiv.org/pdf/2305.02080.pdf)
- StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction (WACV 2024) [[Paper]](https://arxiv.org/pdf/2308.12570.pdf) [[Github]](https://github.com/yuantianyuan01/StreamMapNet)
- PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.16477.pdf)
- Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach (ICCV 2023) [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf)
- TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.06753.pdf) [[Github]](https://github.com/wudongming97/TopoMLP)
- ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction (CoRL 2023) [[Paper]](https://arxiv.org/pdf/2310.13378.pdf) [[Github]](https://github.com/jingy1yu/ScalableMap)
- Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.10517.pdf)
- Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.04079.pdf) [[Github]](https://github.com/NVlabs/SMERF)
- P-MAPNET: FAR-SEEING MAP CONSTRUCTOR ENHANCED BY BOTH SDMAP AND HDMAP PRIORS (ICLR 2024 submitted paper) [[Openreview]](https://openreview.net/forum?id=lgDrVM9Rpx) [[Paper]](https://openreview.net/pdf?id=lgDrVM9Rpx)
- Online Vectorized HD Map Construction using Geometry (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.03341.pdf) [[Github]](https://github.com/cnzzx/GeMap)
- LANESEGNET: MAP LEARNING WITH LANE SEGMENT PERCEPTION FOR AUTONOMOUS DRIVING (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.16108.pdf) [[Github]](https://github.com/OpenDriveLab/LaneSegNet)
- 3D Lane Detection from Front or Surround-View using Joint-Modeling & Matching (Arxiv 2024) [[Paper](https://arxiv.org/pdf/2401.08036.pdf)
- MapNeXt: Revisiting Training and Scaling Practices for Online Vectorized HD Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.07323.pdf)
- Stream Query Denoising for Vectorized HD Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.09112.pdf)
- ADMap: Anti-disturbance framework for reconstructing online vectorized HD map (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.13172.pdf)
- PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction in High-definition Maps (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.14024.pdf)
- LaneGraph2Seq: Lane Topology Extraction with Language Model via Vertex-Edge Encoding and Connectivity Enhancement (AAAI 2024) [[paper]](https://arxiv.org/pdf/2401.17609.pdf)
- VI-Map: Infrastructure-Assisted Real-Time HD Mapping for Autonomous Driving (Arxiv 2024) [[Paper]](https://yanzhenyu.com/assets/pdf/VI-Map-MobiCom23.pdf)
- CurveFormer++: 3D Lane Detection by Curve Propagation with Temporal Curve Queries and Attention (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.06423)
- VI-Map: Infrastructure-Assisted Real-Time HD Mapping for Autonomous Driving (Arxiv 2024) [[paper]](https://yanzhenyu.com/assets/pdf/VI-Map-MobiCom23.pdf)
- Lane2Seq: Towards Unified Lane Detection via Sequence Generation (CVPR 2024) [[Paper]](https://arxiv.org/abs/2402.17172)
- Leveraging Enhanced Queries of Point Sets for Vectorized Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.17430) [[Github]](https://github.com/HXMap/MapQR)
- MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.15951) [[Github]](https://map-tracker.github.io/)
- Producing and Leveraging Online Map Uncertainty in Trajectory Prediction (CVPR 2024) [[Paper]](https://arxiv.org/abs/2403.16439) [[Github]](https://github.com/alfredgu001324/MapUncertaintyPrediction)
- MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction (CVPR 2024) [[Paper]](https://arxiv.org/abs/2404.00876) [[Github]](https://github.com/xiaolul2/MGMap)
- HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction (CVPR 2024) [[Paper]](https://arxiv.org/abs/2403.08639)
- SemVecNet: Generalizable Vector Map Generation for Arbitrary Sensor Configurations (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.00250)
- DTCLMapper: Dual Temporal Consistent Learning for Vectorized HD Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.05518)
- Addressing Diverging Training Costs using Local Restoration for Precise Bird's Eye View Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.01016)
- Is Your HD Map Constructor Reliable under Sensor Corruptions? (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.12214) [[Github]](https://github.com/mapbench/toolkit) [[Project]](https://mapbench.github.io/)
- DuMapNet: An End-to-End Vectorization System for City-Scale Lane-Level Map Generation（KDD 2024）[[Paper]](https://arxiv.org/abs/2406.14255)
- LGmap: Local-to-Global Mapping Network for Online Long-Range Vectorized HD Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.13988)
- Accelerating Online Mapping and Behavior Prediction via Direct BEV Feature Attention (ECCV 2024) [[Paper]](https://arxiv.org/pdf/2407.06683) [[Github]](https://github.com/alfredgu001324/MapBEVPrediction)
- BLOS-BEV: Navigation Map Enhanced Lane Segmentation Network, Beyond Line of Sight (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.08526)
- Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using Large-scale Public Data (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.08726)
- MapDistill: Boosting Efficient Camera-based HD Map Construction via Camera-LiDAR Fusion Model Distillation (ECCV 2024) [[Paper]](https://arxiv.org/abs/2407.11682)
- Mask2Map: Vectorized HD Map Construction Using Bird's Eye View Segmentation Masks (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.13517) [[Github]](https://github.com/SehwanChoi0307/Mask2Map)
- Generation of Training Data from HD Maps in the Lanelet2 Framework (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.17409)
- PrevPredMap: Exploring Temporal Modeling with Previous Predictions for Online Vectorized HD Map Construction (Arxiv 2024) [[paper]](https://arxiv.org/abs/2407.17378) [[Github]](https://github.com/pnnnnnnn/PrevPredMap)
- CAMAv2: A Vision-Centric Approach for Static Map Element Annotation (Arxiv 2024) [[Paper]](/https://arxiv.org/pdf/2407.21331)
- HeightLane: BEV Heightmap guided 3D Lane Detection (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2408.08270)
- PriorMapNet: Enhancing Online Vectorized HD Map Construction with Priors (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.08802)
- Local map Construction Methods with SD map: A Novel Survey (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.02415)
- Enhancing Vectorized Map Perception with Historical Rasterized Maps (ECCV 2024) [[Paper]](https://arxiv.org/abs/2409.00620) [[Github]](https://github.com/HXMap/HRMapNet)
- GenMapping: Unleashing the Potential of Inverse Perspective Mapping for Robust Online HD Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.08688v1) [[Github]](https://github.com/lynn-yu/GenMapping?tab=readme-ov-file)
- GlobalMapNet: An Online Framework for Vectorized Global HD Map Construction (Arxiv 2024) [[paper]] (https://arxiv.org/abs/2409.10063)
- MemFusionMap: Working Memory Fusion for Online Vectorized HD Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.18737)
- MGMapNet: Multi-Granularity Representation Learning for End-to-End Vectorized HD Map Construction (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2410.07733)
- Exploring Semi-Supervised Learning for Online Mapping (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2410.10279)
- OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.23278) [[Github]](https://github.com/bjzhb666/get_google_maps_image) [[Project]](https://opensatmap.github.io/)
- HeightMapNet: Explicit Height Modeling for End-to-End HD Map Learning (WACV 2025) [[Paper]](https://arxiv.org/abs/2411.01408) [[Github]](https://github.com/adasfag/HeightMapNet/)
- M3TR: Generalist HD Map Construction with Variable Map Priors (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.10316) [[Github]](https://github.com/immel-f/m3tr)
- TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.14751)
- ImagineMap: Enhanced HD Map Construction with SD Maps (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.16938)
- Anchor3DLane++: 3D Lane Detection via Sample-Adaptive Sparse 3D Anchor Regression (TPAMI 2025) [[paper]](https://arxiv.org/abs/2412.16889) [[Github]](https://github.com/tusen-ai/Anchor3DLane)
- LDMapNet-U: An End-to-End System for City-Scale Lane-Level Map Updating (KDD 2025) [[Paper]](https://arxiv.org/pdf/2501.02763)
- MapGS: Generalizable Pretraining and Data Augmentation for Online Mapping via Novel View Synthesis (Arxiv 2025) [[paper]](https://arxiv.org/pdf/2501.06660)
- Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2502.08974)
- Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2502.10127)
- FastMap: Fast Queries Initialization Based Vectorized HD Map Reconstruction Framework (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.05492) [[Github]](https://github.com/hht1996ok/FastMap)
- Chameleon: Fast-slow Neuro-symbolic Lane Topology Extraction (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.07485)
- HisTrackMap: Global Vectorized High-Definition Map Construction via History Map Tracking (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.07168)
- AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.13430)
### Lanegraph
#### Monocular
- Lane Graph Estimation for Scene Understanding in Urban Driving (IEEE RAL 2021) [[Paper]](https://arxiv.org/abs/2105.00195)
- AutoGraph: Predicting Lane Graphs from Traffic Observations (IEEE RAL 2023) [[Paper]](https://arxiv.org/abs/2306.15410)
- Learning and Aggregating Lane Graphs for Urban Automated Driving (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2302.06175.pdf)
- TopoLogic: An Interpretable Pipeline for Lane Topology Reasoning on Driving Scenes (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2405.14747)
- Enhancing 3D Lane Detection and Topology Reasoning with 2D Lane Priors (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.03105)
- Learning Lane Graphs from Aerial Imagery Using Transformers (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.05687)
- TopoMaskV2: Enhanced Instance-Mask-Based Formulation for the Road Topology Problem (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.11325)
- LMT-Net: Lane Model Transformer Network for Automated HD Mapping from Sparse Vehicle Observations (ITSC 2024) [[Paper]](https://arxiv.org/abs/2409.12409)
- Behavioral Topology (BeTop), a multi-agent behavior formulation for interactive motion prediction and planning (NeurIPS 2024) [[Paper]](https://arxiv.org/abs/2409.18031) [[Github]](https://github.com/OpenDriveLab/BeTop)
- SMART: Advancing Scalable Map Priors for Driving Topology Reasoning (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2502.04329) [[Project]](https://jay-ye.github.io/smart/)
### Tracking
- Exploring Point-BEV Fusion for 3D Point Cloud Object Tracking with Transformer (Arxiv 2022) [[Paper]](https://arxiv.org/pdf/2208.05216.pdf) [[Github]](https://github.com/Jasonkks/PTTR)
- EarlyBird: Early-Fusion for Multi-View Tracking in the Bird's Eye View (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.13350.pdf) [[Github]](https://github.com/tteepe/EarlyBird)
- Traj-MAE: Masked Autoencoders for Trajectory Prediction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.06697)
- Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.20743)
- MapsTP: HD Map Images Based Multimodal Trajectory Prediction for Automated Vehicles (Arixv 2024) [[Paper]](https://arxiv.org/pdf/2407.05811)
- Perception Helps Planning: Facilitating Multi-Stage Lane-Level Integration via Double-Edge Structures (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.11644)
- Hierarchical and Decoupled BEV Perception Learning Framework for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.12491)
- VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions (Arxiv 2024) [[Paper]](https://moonseokha.github.io/VisionTrap/)
### Locate
- BEV-Locator: An End-to-end Visual Semantic Localization Network Using Multi-View Images (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2211.14927.pdf)
- BEV-SLAM: Building a Globally-Consistent WorldMap Using Monocular Vision (IROS 2022) [[Paper]](https://cvssp.org/Personal/OscarMendez/papers/pdf/RossIROS2022.pdf)
- U-BEV: Height-aware Bird’s-Eye-View Segmentation and Neural Map-based Relocalization (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.13766.pdf)
- Monocular Localization with Semantics Map for Autonomous Vehicles (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.03835)
### Occupancy Prediction
- Semantic Scene Completion from a Single Depth Image (CVPR 2017) [[Paper]](https://arxiv.org/pdf/1611.08974.pdf)
- Occupancy Networks: Learning 3D Reconstruction in Function Space (CVPR 2019) [[Paper]](https://arxiv.org/pdf/1812.03828.pdf) [[Github]](https://avg.is.mpg.de/publications/occupancy-networks)
- S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds (CoRL 2020) [[Paper]](https://arxiv.org/pdf/2012.09242.pdf)
- 3D Semantic Scene Completion: a Survey (IJCV 2021) [[Paper]](https://arxiv.org/pdf/2103.07466.pdf)
- Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data (Arxiv 2021) [[Paper]](https://arxiv.org/pdf/2011.09141.pdf)
- Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion (AAAI 2021) [[Paper]](https://arxiv.org/pdf/2012.03762.pdf)
- Anisotropic Convolutional Networks for 3D Semantic Scene Completion (CVPR 2020) [[Paper]](https://arxiv.org/pdf/2004.02122.pdf)
- Estimation  of  Appearance  and  Occupancy  Information  in  Bird’s  EyeView  from  Surround  Monocular  Images (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2211.04557.pdf) [[Project]](https://uditsinghparihar.github.io/APP_OCC/)
- Semantic Segmentation-assisted Scene Completion for LiDAR Point Clouds (IROS 2021) [[Paper]](https://arxiv.org/pdf/2109.11453.pdf) [[Github]](https://github.com/jokester-zzz/SSA-SC)
- Grid-Centric Traffic Scenario Perception for Autonomous Driving: A Comprehensive Review (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.01212.pdf)
- LMSCNet: Lightweight Multiscale 3D Semantic Completion (IC 3DV 2020) [[Paper]](https://arxiv.org/pdf/2008.10559.pdf) [[[Github]](https://github.com/astra-vision/LMSCNet)
- MonoScene: Monocular 3D Semantic Scene Completion (CVPR 2022) [[Paper]](https://arxiv.org/pdf/2112.00726.pdf) [[Github]](https://github.com/astra-vision/MonoScene) [[Project]](https://astra-vision.github.io/MonoScene/)
- OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2304.05316.pdf) [[Github]](https://github.com/zhangyp15/OccFormer)
- A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.10076.pdf) [[Github]](https://github.com/GANWANSHUI/SimpleOccupancy)
- OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2302.13540.pdf) [[Github]](https://github.com/megvii-research/OccDepth)
- OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.03991.pdf) [[Github]](https://github.com/JeffWang987/OpenOccupancy)
- Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.14365.pdf) [[Github]](https://github.com/Tsinghua-MARS-Lab/Occ3D) [[Project]](https://tsinghua-mars-lab.github.io/Occ3D/)
- Occ-BEV: Multi-Camera Unified Pre-training via 3DScene Reconstruction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.18829.pdf) [[Github]](https://github.com/chaytonmin/Occ-BEV)
- StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.13959.pdf) [[Github]](https://github.com/Arlo0o/StereoScene)
- Learning Occupancy for Monocular 3D Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.15694.pdf) [[Github]](https://github.com/SPengLiang/OccupancyM3D)
- OVO: Open-Vocabulary Occupancy (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.16133.pdf)
- SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2303.09551.pdf) [[Github]](https://github.com/weiyithu/SurroundOcc) [[Project]](https://weiyithu.github.io/SurroundOcc/)
- Scene as Occupancy (Arxiv 2023) [[Paper]]](https://arxiv.org/pdf/2306.02851.pdf) [[Github]](https://github.com/OpenDriveLab/OccNet)
- Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2301.00527.pdf) [[Github]](https://github.com/zoomin-lee/scene-scale-diffusion)
- PanoOcc: Unified Occupancy Representation for Camera-based3D Panoptic Segmentation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2306.10013.pdf) [[Github]](https://github.com/Robertwyq/PanoOcc)
- UniOcc: Unifying Vision-Centric 3D Occupancy Predictionwith Geometric and Semantic Rendering (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2306.09117.pdf)
- SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving (NeurIPS 2023 D&B track) [[paper]](https://arxiv.org/pdf/2306.09001.pdf) [[paper]](https://github.com/ai4ce/SSCBench)
- StereoVoxelNet:  Real-Time  Obstacle  Detection  Based  on  OccupancyVoxels  from  a  Stereo  Camera  Using  Deep  Neural  Networks (ICRA 2023) [[Paper]] (https://arxiv.org/pdf/2209.08459.pdf) [[Github]](https://github.com/RIVeR-Lab/stereovoxelnet) [[Project]](https://lhy.xyz/stereovoxelnet/)
- Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2302.07817.pdf) [[Github]](https://github.com/wzzheng/TPVFormer)
- VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction (CVPR 2023) [[paper]](https://arxiv.org/pdf/2302.12251.pdf) [[Github]](https://github.com/NVlabs/VoxFormer)
- Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting (CVPR 2023) [[Paper]](/https://arxiv.org/pdf/2302.13130.pdf) [[Github]](https://www.cs.cmu.edu/~tkhurana/ff4d/index.html) [[Project]](https://github.com/tarashakhurana/4d-occ-forecasting)
- SSCBench: A Large-Scale 3D Semantic SceneCompletion Benchmark for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2306.09001.pdf) [[Github]](https://github.com/ai4ce/SSCBench)
- SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion (IROS 2023) [[Paper]](https://arxiv.org/pdf/2306.15349.pdf) [[Github]](https://github.com/Jieqianyu/SSC-RS)
- CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2307.07938.pdf)
- Symphonize 3D Semantic Scene Completion with Contextual Instance Queries (Arxiv 2023) [[Paper]](/https://arxiv.org/pdf/2306.15670.pdf) [[Github]](https://github.com/hustvl/Symphonies)
- Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2206.09900.pdf)
- UniWorld: Autonomous Driving Pre-training via World Models (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.07234.pdf) [[Github]](https://github.com/chaytonmin/UniWorld)
- PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2308.16896.pdf) [[Github]](https://github.com/wzzheng/PointOcc)
- SOGDet: Semantic-Occupancy Guided Multi-view 3D Object Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2308.13794.pdf) [[Github]](https://github.com/zhouqiu/SOGDet)
- OccupancyDETR: Making Semantic Scene Completion as Straightforward as Object Detection (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.08504.pdf) [[Github]](https://github.com/jypjypjypjyp/OccupancyDETR)
- PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.12708.pdf)
- SPOT: SCALABLE 3D PRE-TRAINING VIA OCCUPANCY PREDICTION FOR AUTONOMOUS DRIVING (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.10527.pdf)
- NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space (Arxiv 2023) [[Github]](https://github.com/Jiawei-Yao0812/NDCScene)
- Anisotropic Convolutional Networks for 3D Semantic Scene Completion (CVPR 2020) [[Github]](https://github.com/waterljwant/SSC) [[Project]](https://waterljwant.github.io/SSC/)
- RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.09502.pdf) [[Github]](https://github.com/pmj110119/RenderOcc)
- LiDAR-based 4D Occupancy Completion and Forecasting (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.11239.pdf) [[Github]](https://github.com/ai4ce/Occ4cast)
- SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.11371.pdf)
- SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.12754.pdf) [[Github]](https://github.com/huang-yh/SelfOcc)
- FlashOcc: Fast and Memory-Efficient Occupancy Prediction via Channel-to-Height Plugin (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.12058.pdf)
- Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.17663.pdf) [[Github]](https://github.com/haomo-ai/Cam4DOcc)
- OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.16038.pdf) [[Github]](https://github.com/wzzheng/OccWorld)
- DepthSSC: Depth-Spatial Alignment and Dynamic Voxel Resolution for Monocular 3D Semantic Scene Completion (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.17084.pdf)
- A Simple Framework for 3D Occupancy Estimation in Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.10076.pdf) [[Github]](https://github.com/GANWANSHUI/SimpleOccupancy)
- OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.03774.pdf)
- COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.01919.pdf)
- OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.09243.pdf) [[Github]](https://github.com/LinShan-Bin/OccNeRF)
- RadOcc: Learning Cross-Modality Occupancy Knowledge through Rendering Assisted Distillation (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.11829.pdf)
- PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.02158.pdf) [[Project]](https://astra-vision.github.io/PaSCo/) [[Github]](https://github.com/astra-vision/PaSCo)
- POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.09413.pdf) [[Github]](https://arxiv.org/pdf/2401.09413.pdf)
- S2TPVFormer: Spatio-Temporal Tri-Perspective View for temporally coherent 3D Semantic Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.13785.pdf)
- InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.12422.pdf)
- V2VSSC: A 3D Semantic Scene Completion Benchmark for Perception with Vehicle to Vehicle Communication (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2402.04671.pdf)
- OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.12792)
- OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.01644)
- OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.18140)
- FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View (ICRA 2024) [[Paper]](https://arxiv.org/abs/2403.02710)
- OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.05329)
- PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness (CVPR 2024) [[Paper]](https://arxiv.org/abs/2212.02501) [[Github]](https://github.com/astra-vision/PaSCo?tab=readme-ov-file)
- Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.08748)
- OccFiner: Offboard Occupancy Refinement with Hybrid Propagation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.08504)
- MonoOcc: Digging into Monocular Semantic Occupancy Prediction (ICLR 2024) [[Paper]](https://arxiv.org/abs/2403.08766)
- OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.11796)
- Urban Scene Diffusion through Semantic Occupancy Map (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.11697)
- Co-Occ: Coupling Explicit Feature Fusion with Volume Rendering Regularization for Multi-Modal 3D Semantic Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2404.04561) [[Github]](https://rorisis.github.io/Co-Occ_project-page/)
- SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction (CVPR 2024) [[Paper]](https://arxiv.org/abs/2404.09502)
- Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with Self-Distillation (CVPR 2024) [[paper]](https://arxiv.org/abs/2404.11958) [[Github]](https://github.com/songw-zju/HASSC)
- OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining BEV Segmentation Networks (Arxiv 2023) [[Paper]](https://arxiv.org/abs/2404.13046)
- OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2404.15014)
- ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D Occupancy Perception via View-Guided Transformers (Arxiv 2024) [[paper]](https://arxiv.org/abs/2405.04299)
- A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.05173)
- Vision-based 3D occupancy prediction in autonomous driving: a review and outlook (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.02595)
- GEOcc: Geometrically Enhanced 3D Occupancy Network with Implicit-Explicit Depth Fusion and Contextual Self-Supervision (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.10591)
- RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar (Arxiv 2024) [[paper]](https://arxiv.org/abs/2405.14014)
- GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.17429) [[Github]](https://github.com/huang-yh/GaussianFormer)
- OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.20337) [[Github]](https://github.com/wzzheng/OccSora)
- EFFOcc: A Minimal Baseline for EFficient Fusion-based 3D Occupancy Network (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.07042) [[Github]](https://github.com/synsin0/EFFOcc)
- PanoSSC: Exploring Monocular Panoptic 3D Scene Reconstruction for Autonomous Driving (3DV 2024) [[paper]](https://arxiv.org/abs/2406.07037)
- UnO: Unsupervised Occupancy Fields for Perception and Forecasting (Arxiv 2024) [[paper]](https://arxiv.org/abs/2406.08691)
- Context and Geometry Aware Voxel Transformer for Semantic Scene Completion (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.13675) [[Github]](https://github.com/pkqbajng/CGFormer?tab=readme-ov-file)
- Occupancy as Set of Points (ECCV 2024) [[Paper]](https://arxiv.org/abs/2407.04049) [[Github]](https://github.com/hustvl/osp)
- Lift, Splat, Map: Lifting Foundation Masks for Label-Free Semantic Scene Completion (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.03425)
- Let Occ Flow: Self-Supervised 3D Occupancy Flow Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.07587)
- Monocular Occupancy Prediction for Scalable Indoor Scenes (ECCV 2024) [[Paper]](https://arxiv.org/abs/2407.11730) [[Github]](https://github.com/hongxiaoy/ISO)
- LangOcc: Self-Supervised Open Vocabulary Occupancy Estimation via Volume Rendering (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.17310)
- VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy Prediction (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2408.03551)
- Vision-Language Guidance for LiDAR-based Unsupervised 3D Object Detection (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2408.03790) [[Github]](https://github.com/chreisinger/ViLGOD)
- OccMamba: Semantic Occupancy Prediction with State Space Models (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2408.09859)
- HybridOcc: NeRF Enhanced Transformer-based Multi-Camera 3D Occupancy Prediction (IEEE RAL 2024) [[paper]](https://arxiv.org/abs/2408.09104)
- Semi-supervised 3D Semantic Scene Completion with 2D Vision Foundation Model Guidance (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2408.11559)
- MambaOcc: Visual State Space Model for BEV-based Occupancy Prediction with Local Adaptive Reordering (Arxiv 2024) [[paper]](https://arxiv.org/abs/2408.11464) [[Project]](https://ganwanshui.github.io/GaussianOcc/) [[Github]](https://github.com/Hub-Tian/MambaOcc)
- GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting (Arxiv 2024) [[paper]](https://ganwanshui.github.io/GaussianOcc/) [[Github]](https://github.com/GANWANSHUI/GaussianOcc)
- AdaOcc: Adaptive-Resolution Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.13454)
- Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.14846)
- UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D Object Detection via Channel2height (Arxiv 2024) [[paper]](https://arxiv.org/abs/2409.11160)
- COCO-Occ: A Benchmark for Occluded Panoptic Segmentation and Image Understanding (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.12760)
- CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction (ECCV 2024) [[Paper]](https://arxiv.org/abs/2409.13430) [[Github]](https://github.com/Tsinghua-MARS-Lab/CVT-Occ)
- ReliOcc: Towards Reliable Semantic Occupancy Prediction via Uncertainty Learning (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.18026)
- DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.18092)
- SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.00337)
- OccRWKV: Rethinking Efficient 3D Semantic Occupancy Prediction with Linear Complexity (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.19987) [[Github]](https://github.com/jmwang0117/OccRWKV) [[Project]](https://jmwang0117.github.io/OccRWKV/)
- DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.19972) [[Github]](https://github.com/AlphaPlusTT/DAOcc)
- OCC-MLLM:Empowering Multimodal Large Language Model For the Understanding of Occluded Objects (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.01261)
- OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction (Arxiv 2024) [[paper]](https://arxiv.org/abs/2411.03696)
- Spatiotemporal Decoupling for Efficient Vision-Based Occupancy Forecasting(Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.14169)
- ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2411.19548) [[Github]](https://github.com/GigaAI-research/ReconDreamer) [[Project]](https://recondreamer.github.io/)
- EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.04380) [[Github]](https://github.com/YkiWu/EmbodiedOcc)
- PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.07616)
- Fast Occupancy Network (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.07163)
- Lightweight Spatial Embedding for Vision-based 3D Occupancy Prediction (ARxiv 2024) [[Paper]](https://arxiv.org/abs/2412.05976)
- doScenes: An Autonomous Driving Dataset with Natural Language Instruction for Human Interaction and Vision-Language Navigation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.05893) [[Github]](https://github.com/rossgreer/doScenes)
- LOMA: Language-assisted Semantic Occupancy Network via Triplane Mamba (AAAI 2025) [[paper]](https://arxiv.org/abs/2412.08388)
- GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.10373) [[Github]](https://github.com/zuosc19/GaussianWorld)
- ViPOcc: Leveraging Visual Priors from Vision Foundation Models for Single-View 3D Occupancy Prediction (AAAI 2025) [[Paper]](https://arxiv.org/abs/2412.11210) [[Github]](https://github.com/fengyi233/ViPOcc) [[Github]](https://mias.group/ViPOcc/)
- OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D Scene Generation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.11183)
- ProtoOcc: Accurate, Efficient 3D Occupancy Prediction Using Dual Branch Encoder-Prototype Query Decoder (AAAI 2025 Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.08774) [[Paper]](https://github.com/SPA-junghokim/ProtoOcc)
- MR-Occ: Efficient Camera-LiDAR 3D Semantic Occupancy Prediction Using Hierarchical Multi-Resolution Voxel Representation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.20480)
- Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion (AAAI 2025) [[Paper]](https://arxiv.org/pdf/2501.07260)
- Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2501.15394)
- MetaOcc: Surround-View 4D Radar and Camera Fusion Framework for 3D Occupancy Prediction with Dual Training Strategies (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2501.15384) [[Github]](https://github.com/LucasYang567/MetaOcc)
- OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and Geometric-Aware Gaussian Splatting (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2502.04981)
- MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for Traffic Monitoring (Arxiv 2025) [[paper]](https://arxiv.org/abs/2502.11287)
- OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving (ARxiv 2025) [[Paper]](https://arxiv.org/abs/2502.14235)
- LEAP: Enhancing Vision-Based Occupancy Networks with Lightweight Spatio-Temporal Correlation (Arxiv 2025) [[Paper]]](https://arxiv.org/abs/2502.15438)
- OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework (Arxiv 2025) [[paper]](https://arxiv.org/abs/2502.15180)
- GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.17288)
- H3O: Hyper-Efficient 3D Occupancy Prediction with Heterogeneous Supervision (ICRA 2025) [[Paper]](https://arxiv.org/abs/2503.04059)
- TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction via Spatio-Temporal Gaussian Splatting (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.08485)
- OCCUQ: Exploring Efficient Uncertainty Quantification for 3D Occupancy Prediction (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.10605) [[Github]](https://github.com/ika-rwth-aachen/OCCUQ)
- L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model (ARxiv 2025) [[Paper]](https://arxiv.org/abs/2503.12369) [[Project]](https://studyingfufu.github.io/L2COcc/) [[Github]](https://github.com/StudyingFuFu/L2COcc/tree/master)
- 3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation (CVPR 2025) [[Paper]](https://arxiv.org/abs/2503.15185)
- SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.16399)
#### Occupancy Challenge
- FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation (CVPR 2023 3D Occupancy Prediction Challenge WorkShop) [[paper]](https://arxiv.org/pdf/2307.01492.pdf) [[Github]](https://github.com/NVlabs/FB-BEV)
- Separated RoadTopoFormer (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.01557.pdf)
- OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios (CVPR 2023 WorkShop) [[Paper]](https://arxiv.org/pdf/2307.10934.pdf) [[Github]](https://drive.google.com/file/d/1IFUxbx1hI7iA7uXxilfq-Z0JXMGEU2Zb/view)
- AdaOcc: Adaptive Forward View Transformation and Flow Modeling for 3D Occupancy and Flow Prediction (CVPR 2024 Workshop) [[Paper]](https://arxiv.org/pdf/2407.01436)
- Real-Time 3D Occupancy Prediction via Geometric-Semantic Disentanglement (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.13155)
### Challenge
- The 1st-place Solution for CVPR 2023 OpenLane Topologyin Autonomous Driving Challenge [[Paper]](https://arxiv.org/pdf/2306.09590.pdf)
- MapVision: CVPR 2024 Autonomous Grand Challenge Mapless Driving Tech Report (CVPR 2024 Challenge) [[Paper]](https://arxiv.org/abs/2406.10125)
### Dataset
- Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark (CVPR 2023) [[paper]](https://arxiv.org/pdf/2212.08914.pdf) [[Github]](https://github.com/JeffWang987/ASAP)
- SemanticSpray++: A Multimodal Dataset for Autonomous Driving in Wet Surface Conditions (IV 2024) [[Paper]](https://arxiv.org/abs/2406.09945) [[Project]](https://semantic-spray-dataset.github.io/) [[Github]](https://github.com/uulm-mrm/semantic_spray_dataset)
- WayveScenes101: A Dataset and Benchmark for Novel View Synthesis in Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2407.08280) [[Project]](https://wayve.ai/science/wayvescenes101/) [[Github]](https://github.com/wayveai/wayve_scenes)
- WildOcc: A Benchmark for Off-Road 3D Semantic Occupancy Prediction (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2410.15792) [[Github]](https://github.com/LedKashmir/WildOcc)
### World Model
- End-to-end Autonomous Driving: Challenges and Frontiers (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2306.16927.pdf) [[Github]](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
- Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving (ICRA 2024) [[paper]](https://arxiv.org/pdf/2310.02251.pdf) [[Github]](https://github.com/llmbev/talk2bev) [[Project]](https://llmbev.github.io/talk2bev/)
- Language Prompt for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/abs/2309.04379) [[Github]](https://github.com/wudongming97/Prompt4Driving)
- MotionLM: Multi-Agent Motion Forecasting as Language Modeling (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.16534.pdf)
- GAIA-1: A Generative World Model for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.17080.pdf)
- DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.09777.pdf)
- Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.01957.pdf) [[Github]](https://github.com/wayveai/driving-with-llms)
- Learning to Drive Anywhere (CORL 2023) [[Paper]](https://arxiv.org/pdf/2309.12295.pdf)
- Language-Conditioned Path Planning (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2308.16893.pdf)
- DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model (Arxiv 2023) [[Paper]](https://browse.arxiv.org/pdf/2310.01412.pdf) [[Project]](https://tonyxuqaq.github.io/projects/DriveGPT4/)
- GPT-Driver: Learning to Drive with GPT (Arxiv 2023) [[Paper]](https://browse.arxiv.org/pdf/2310.01415.pdf)
- LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/abs/2310.03026)
- TOWARDS END-TO-END EMBODIED DECISION MAKING VIA MULTI-MODAL LARGE LANGUAGE MODEL: EXPLORATIONS WITH GPT4-VISION AND BEYOND (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.02071.pdf)
- DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.07771.pdf)
- UNIPAD: A UNIVERSAL PRE-TRAINING PARADIGM FOR AUTONOMOUS DRIVING (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.08370.pdf) [[Github]](https://github.com/Nightmare-n/UniPAD)
- PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.08586.pdf)
- Uni3D: Exploring Unified 3D Representation at Scale (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.06773.pdf) [[Github]](https://github.com/baaivision/Uni3D)
- Video Language Planning (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.10625.pdf) [[Github]](https://video-language-planning.github.io/)
- RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.10221.pdf)
- DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.12128.pdf) [[Paper]](https://github.com/aszala/DiagrammerGPT) [[Project]](https://diagrammergpt.github.io/)
- Vision Language Models in Autonomous Driving and Intelligent Transportation Systems (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.14414.pdf)
- ADAPT: Action-aware Driving Caption Transformer (ICRA 2023) [[Paper]](https://arxiv.org/pdf/2302.00673.pdf) [[Github]](https://github.com/jxbbb/ADAPT)
- Language Prompt for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.04379.pdf) [[Github]](https://github.com/wudongming97/Prompt4Driving)
- Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.17642.pdf) [[Project]](https://drive-anywhere.github.io/)
- LEARNING UNSUPERVISED WORLD MODELS FOR AUTONOMOUS DRIVING VIA DISCRETE DIFFUSION (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.01017.pdf)
- ADriver-I: A General World Model for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.13549.pdf)
- HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.05186.pdf)
- On the Road with GPT-4V(vision): Early Explorations of Visual-Language Model on Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.05332.pdf)
- GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.12631.pdf)
- Applications of Large Scale Foundation Models for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2311.12144v5.pdf)
- Dolphins: Multimodal Language Model for Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.00438.pdf) [[Project]](https://vlm-driver.github.io/)
- Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.17918.pdf) [[Github]](https://github.com/BraveGroup/Drive-WM) [[Project]](https://drive-wm.github.io/)
- Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving? (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.03031.pdf) [[Github]](https://github.com/NVlabs/BEV-Planner)
- NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.06352.pdf) [[Github]](https//github.com/turingmotors/NuScenes-MQA)
- DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.09245.pdf) [[[Github]](https://github.com/OpenGVLab/DriveMLM)
- DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.07920.pdf) [[Project]](https://pkuvdig.github.io/DrivingGaussian/)
- Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.03661.pdf) [[Github]](https://github.com/fudan-zvg/Reason2Drive)
- Dialogue-based generation of self-driving simulation scenarios using Large Language Models (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2310.17372.pdf) [[Github]](https://github.com/avmb/dialogllmscenic?tab=readme-ov-file)
- Panacea: Panoramic and Controllable Video Generation for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.16813.pdf) [[Project]](https://panacea-ad.github.io/) [[Github]](https://github.com/wenyuqing/panacea)
- LingoQA: Video Question Answering for Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.14115.pdf) [[Github]](https://github.com/wayveai/LingoQA/)
- DriveLM: Driving with Graph Visual Question Answering (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.14150.pdf) [[Github]](https://github.com/OpenDriveLab/DriveLM)
- LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.14074.pdf) [[Project]](https://sites.google.com/view/lidar-llm)
- LMDrive: Closed-Loop End-to-End Driving with Large Language Models (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.07488v2.pdf) [[Github]](https://github.com/opendilab/LMDrive)
- Visual Point Cloud Forecasting enables Scalable Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.17655.pdf) [[Github]](https://github.com/OpenDriveLab/ViDAR)
- WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.02934.pdf) [[Github]](https://github.com/fudan-zvg/WoVoGen)
- Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.00988.pdf) [[Github]](https://github.com/xmed-lab/NuInstruct)
- DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.03641.pdf)
- A Survey on Multimodal Large Language Models for Autonomous Driving (WACVW 2024) [[Paper]](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf)
- VLP: Vision Language Planning for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2401.05577.pdf)
- Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.08045.pdf)
- MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.07314.pdf)
- Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2402.05746.pdf)
- DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.12289) [[Github]](https://tsinghua-mars-lab.github.io/DriveVLM/)
- GenAD: Generative End-to-End Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.11502) [[Github]](https://arxiv.org/abs/2402.11502)
- Generalized Predictive Model for Autonomous Driving (CVPR 2024) [[Paper]](https://arxiv.org/abs/2403.09630)
- AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.13331)
- DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.16996)
- SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.19438) [[Project]](https://subjectdrive.github.io/)
- DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.06845) [[Project]](https://drivedreamer2.github.io/) [[Github]](https://github.com/f1yfisher/DriveDreamer2)
- DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models (ICLR 2024) [[Paper]](https://arxiv.org/abs/2309.16292) [[Paper]](https://pjlab-adg.github.io/DiLu/)
- OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.01533)
- GAD-Generative Learning for HD Map-Free Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.00515)
- Guiding Attention in End-to-End Driving Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.00242)
- Probing Multimodal LLMs as World Models for Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.05956)
- Traj-LLM: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.04909)
- Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving (Arixv 2024) [[Paper]](https://arxiv.org/abs/2405.05258)
- Unified End-to-End V2X Cooperative Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2405.03971)
- DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2405.04390)
- OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.01533)
- GAD-Generative Learning for HD Map-Free Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2405.00515)
- MaskFuser: Masked Fusion of Joint Multi-Modal Tokenization for End-to-End Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2405.07573)
- MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.14475)
- Language-Image Models with 3D Understanding (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2405.03685) [[Project]](https://janghyuncho.github.io/Cube-LLM/)
- Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving? (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.18361)
- GFlow: Recovering 4D World from Monocular Video (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.18426) [[Github]](https://littlepure2333.github.io/GFlow/)
- Benchmarking and Improving Bird's Eye View Perception Robustness in Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.17426) [[Github]](https://github.com/Daniel-xsy/RoboBEV)
- Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2405.17398) [[Github]](https://github.com/OpenDriveLab/Vista) [[Project]](https://vista-demo.github.io/)
- OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.20337) [[Github]](https://github.com/wzzheng/OccSora) [[Project]](https://wzzheng.net/OccSora/)
- DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.03008) [[Github]](https://sled-group.github.io/driVLMe/)
- AD-H: Autonomous Driving with Hierarchical Agents (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.03474)
- Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.03877) [[Github]](https://thinklab-sjtu.github.io/Bench2Drive/)
- A Superalignment Framework in Autonomous Driving with Large Language Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.05651)
- Enhancing End-to-End Autonomous Driving with Latent World Model (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.08481)
- SimGen: Simulator-conditioned Driving Scene Generation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2406.09386)
- Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset (Arxiv 2024) [[paper]](https://arxiv.org/abs/2406.09383) [[Project]](https://ai4ce.github.io/MARS/)
- WonderWorld: Interactive 3D Scene Generation from a Single Image (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2406.09394)
- CarLLaVA: Vision language models for camera-only closed-loop driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2406.10165)
- End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2406.17680)
- CarLLaVA: Vision language models for camera-only closed-loop driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2406.10165)
- BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space (Arxiv 2024) [[Paper]](https://github.com/zympsyche/BevWorld) [[Github]](https://arxiv.org/pdf/2407.05679)
- Exploring the Causality of End-to-End Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2407.06546) [[Github]](https://github.com/bdvisl/DriveInsight)
- SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.21293)
- DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.00415) [[Github]](https://github.com/PJLab-ADG/DriveArena)
- Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.03516)
- Open 3D World in Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.10880)
- CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.10845)
- Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2408.14197)
- DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.16647)
- OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.03272)
- Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.02914)
- ContextVLM: Zero-Shot and Few-Shot Context Understanding for Autonomous Driving using Vision Language Models (ITSC 2024) [[Paper]](https://arxiv.org/abs/2409.00301)
- MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2409.07267)
- RenderWorld: World Model with Self-Supervised 3D Label (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.11356)
- Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.11182)
- DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene Reconstruction from Flexible Surround-view Input (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.12753) [[Project]](https://fangzhou2000.github.io/projects/drivingforward/) [[Github]](https://github.com/fangzhou2000/DrivingForward)
- METDrive: Multi-modal End-to-end Autonomous Driving with Temporal Guidance (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2409.12667)
- DOES END-TO-END AUTONOMOUS DRIVING REALLY NEED PERCEPTION TASKS? (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.18341)
- Learning to Drive via Asymmetric Self-Play (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.18218)
- Uncertainty-Guided Enhancement on Driving Perception System via Foundation Models (Arxiv 2024) [[paper]](https://arxiv.org/abs/2410.01144)
- ScVLM: a Vision-Language Model for Driving Safety Critical Event Understanding (Arxiv) [[Paper]](https://arxiv.org/abs/2410.00982)
- Learning to Drive via Asymmetric Self-Play (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2409.18218)
- HE-Drive: Human-Like End-to-End Driving with Vision Language Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.05051) [[Project]](https://jmwang0117.github.io/HE-Drive/) [[Paper]](https://github.com/jmwang0117/HE-Drive)
- UniDrive: Towards Universal Driving Perception Across Camera Configurations (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.13864) [[Github]](https://github.com/ywyeli/UniDrive)
- DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2410.13571) [[Github]](https://github.com/GigaAI-research/DriveDreamer4D) [[Project]](https://drivedreamer4d.github.io/)
- DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model (NeurIPS 2024) [[Paper]](https://arxiv.org/abs/2410.10738) [[Project]](https://drivingdojo.github.io/) [[Github]](https://github.com/Robertwyq/Drivingdojo)
- EMMA: End-to-End Multimodal Model for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.23262) [[Github]](https://waymo.com/blog/2024/10/introducing-emma/)
- Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.22313) [[Github]](https://github.com/hustvl/Senna)
- MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.13807) [[Github]](https://github.com/flymin/MagicDriveDiT) [[Project]](https://gaoruiyuan.com/magicdrivedit/)
- DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2411.15139) [[Github]](https://github.com/hustvl/DiffusionDrive)
- VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.14716)
- Language Driven Occupancy Prediction (Arxiv 2024) [[paper]](https://arxiv.org/abs/2411.16072) [[Github]](https://github.com/pkqbajng/LOcc)
- Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.03520) [[Project]](https://luhannan.github.io/CogDrivingPage/)
- InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.03934) [[Github]](https://research.nvidia.com/labs/toronto-ai/infinicube/)
- Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.05280) [[Github]](https://github.com/wzzheng/Stag)
- UniMLVG: Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.04842)
- Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.06777) [[Github]](https://github.com/Barrybarry-Smith/Driv3R)
- DriveMM: All-in-One Large Multimodal Model for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.07689) [[Github]](https://github.com/zhijian11/DriveMM)
- Physical Informed Driving World Model (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.08410) [[Github]](https://metadrivescape.github.io/papers_project/DrivePhysica/page.html)
- GPD-1: Generative Pre-training for Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.08643) [[Github]](https://github.com/wzzheng/GPD)
- Doe-1: Closed-Loop Autonomous Driving with Large World Model (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.09627) [[Github]](https://github.com/wzzheng/Doe)
- GaussianAD: Gaussian-Centric End-to-End Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.10371) [[Github]](https://github.com/wzzheng/GaussianAD)
- DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.09043) [[Github]](https://github.com/EnVision-Research/DriveRecon)
- SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout (NeurIPS 2024) [[paper]](https://arxiv.org/pdf/2412.12129)
- Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.14058) [[Github]](https://robovlms.github.io/)
- An Efficient Occupancy World Model via Decoupled Dynamic Flow and Image-assisted Training (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.13772)
- OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.15208) [[paper]](https://github.com/taco-group/OpenEMMA)
- AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.15206) [[Github]](https://github.com/taco-group/AutoTrust)
- VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.14446)
- DriveGPT: Scaling Autoregressive Behavior Models for Driving (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.14415)
- DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.18607)
- UniPLV: Towards Label-Efficient Open-World 3D Scene Understanding by Regional Visual Language Supervision (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2412.18131)
- DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.19505) [[Github]](https://github.com/YvanYin/DrivingWorld)
- AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2501.04969) [[Github]](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release)
- DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2501.04671)
- Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives (Arxiv 2025) [[paper]](https://arxiv.org/abs/2501.04003) [[Github]](https://drive-bench.github.io/)
- Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving (Arxiv 2025) [[paper]](https://arxiv.org/abs/2501.06680)
- Distilling Multi-modal Large Language Models for Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2501.09757)
- A Survey of World Models for Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2501.11260)
- HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation (Arxiv 2025) [[paper]](https://arxiv.org/abs/2501.14729) [[Paper]](https://github.com/LMD0311/HERMES)
- SSF: Sparse Long-Range Scene Flow for Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2501.17821)
- V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.09980)
- MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction (Arxiv 2025) [[paper]](https://arxiv.org/abs/2502.11663)
- The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.10498)
- Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.14917)
- VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.18042)
- VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2502.20108)
- FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering (Arxiv 2025) [[Paper]](FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering)
- BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2503.03074)
- GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.05689) [[Github]](https://github.com/YvanYin/GoalFlow)
- AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.07608) [[Github]](https://github.com/hustvl/AlphaDrive)
- CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.07234)
- CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.08683) [[Github]](https://github.com/cxliu0314/CoLMDriver)
- HiP-AD: Hierarchical and Multi-Granularity Planning with Deformable Attention for Autonomous Driving in a Single Decoder (Arxiv 2025) [[paper]](https://arxiv.org/abs/2503.08612)
- DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.07656)
- SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.09594)
- Post-interactive Multimodal Trajectory Prediction for Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.09366)
- Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.09464)
- DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.10621) [[Github]](https://github.com/ayesha-ishaq/DriveLMM-o1)
- Unlock the Power of Unlabeled Data in Language Driving Model (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.10586)
- Finetuning Generative Trajectory Model with Reinforcement Learning from Human Feedback (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2503.10434)
- DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.11265)
- Active Learning from Scene Embeddings for End-to-End Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.11062)
- Centaur: Robust End-to-End Autonomous Driving with Test-Time Training (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.11650)
- InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.13047) [[Paper]](https://github.com/songruiqi/InsightDrive)
- Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.12820)
- Tracking Meets Large Multimodal Models for Driving Scenario Understanding (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2503.14498)
- ChatBEV: A Visual Language Model that Understands BEV Maps (Arxiv 2025) [[paper]](https://arxiv.org/abs/2503.13938)
- RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving (Arxiv 2025) [[paper]](https://arxiv.org/abs/2503.13861)
- Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning (CVPR 2025) [[Paper]](https://arxiv.org/abs/2503.14182)
### Other
- LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2501.04005) [[Github]](https://ldkong.com/LargeAD)
- X-Drive: Cross-modality consistent multi-sensor data synthesis for driving scenarios (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.01123) [[Github]](https://arxiv.org/abs/2411.01123)
- Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views (AAAI 2021) [[Paper]](https://arxiv.org/pdf/2010.01191.pdf) [[Github]](https://github.com/vincentcartillier/Semantic-MapNet) [[Project]](https://vincentcartillier.github.io/smnet.html) 
- Trans4Map: Revisiting Holistic Bird’s-Eye-View Mapping from EgocentricImages to Allocentric Semantics with Vision Transformers (WACV 2023) [[Paper]](Trans4Map: Revisiting Holistic Bird’s-Eye-View Mapping from EgocentricImages to Allocentric Semantics with Vision Transformers)
- ViewBirdiformer: Learning to recover ground-plane crowd trajectories and ego-motion from a single ego-centric view (Arxiv 2022) [[paper]](https://arxiv.org/pdf/2210.06332.pdf)
- 360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.11910.pdf) [[Github]](https://github.com/jamycheung/360BEV) [[Project]](https://jamycheung.github.io/360BEV.html)
- F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.03651.pdf)
- NVAutoNet: Fast and Accurate 360∘ 3D Visual Perception For Self Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2303.12976.pdf)
- FedBEVT: Federated Learning Bird's Eye View Perception Transformer in Road Traffic Systems (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.01534.pdf)
- Aligning Bird-Eye View Representation of PointCloud Sequences using Scene Flow (IEEE IV 2023) [[Paper]](https://arxiv.org/pdf/2305.02909.pdf) [[Github]](https://github.com/quan-dao/pc-corrector)
- MotionBEV: Attention-Aware Online LiDARMoving Object Segmentation with Bird’s Eye Viewbased Appearance and Motion Features (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.07336.pdf)
- WEDGE: A multi-weather autonomous driving dataset built from generativevision-language models (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.07528.pdf) [[Github]](https://github.com/Infernolia/WEDGE) [[Project]](https://infernolia.github.io/WEDGE/)
- Leveraging BEV Representation for360-degree Visual Place Recognition (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.13814.pdf)
- NMR: Neural Manifold Representation for Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2205.05551.pdf)
- V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer (ECCV 2022) [[Paper]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990106.pdf) [[Github]](https://github.com/DerrickXuNu/v2x-vit)
- DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative3D Object Detection (CVPR 2022) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_DAIR-V2X_A_Large-Scale_Dataset_for_Vehicle-Infrastructure_Cooperative_3D_Object_Detection_CVPR_2022_paper.pdf) [[Github]](https://github.com/AIR-THU/DAIR-V2X)
- Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task (CVPR 2022) [[Paper]](https://arxiv.org/pdf/2203.13608.pdf) [[Github]](https://github.com/liyingying0113/rope3d-dataset-tools) [[Project]](https://thudair.baai.ac.cn/rope)
- A Motion and Accident Prediction Benchmark for V2X Autonomous Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2304.01168.pdf) [[Project]](https://deepaccident.github.io/)
- BEVBert: Multimodal Map Pre-training for Language-guided Navigation (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2212.04385.pdf)
- V2X-Seq: A Large-Scale Sequential Dataset forVehicle-Infrastructure Cooperative Perception and Forecasting (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2305.05938.pdf) [[Github]](https://github.com/AIR-THU/DAIR-V2X-Seq) [[Project]](https://thudair.baai.ac.cn/index)
- BUOL: A Bottom-Up Framework with Occupancy-aware Lifting forPanoptic 3D Scene Reconstruction From A Single Image (CVPR 2023) [[paper]](https://arxiv.org/pdf/2306.00965.pdf) [[Github]](https://github.com/chtsy/buol)
- BEVScope:  Enhancing  Self-Supervised  Depth  Estimation  Leveraging Bird’s-Eye-View  in  Dynamic  Scenarios (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2306.11598.pdf)
- Bird’s-Eye-View Scene Graph for Vision-Language Navigation (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2308.04758.pdf)
- OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2310.13398.pdf)
- Hidden Biases of End-to-End Driving Models (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2306.07957.pdf) [[Github]][https://github.com/autonomousvision/carla_garage]
- EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2307.08991.pdf)
- End-to-end Autonomous Driving: Challenges and Frontiers (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2306.16927.pdf) [[Github]](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
- BEVPlace: Learning LiDAR-based Place Recognition using Bird’s Eye View Images (ICCV 2023) [[paper]](https://arxiv.org/pdf/2302.14325.pdf)
- I2P-Rec: Recognizing Images on Large-scale Point Cloud Maps through Bird’s Eye View Projections (IROS 2023) [[Paper]](https://arxiv.org/pdf/2303.01043.pdf)
- Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.01471.pdf) [[Project]](https://waabi.ai/research/implicito)
- BEV-DG: Cross-Modal Learning under Bird’s-Eye View for Domain Generalization of 3D Semantic Segmentation (ICCV 2023) [[paper]](https://arxiv.org/pdf/2308.06530.pdf)
- MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.12963.pdf) [[Github]](https://github.com/xiyuez2/MapPrior) [[Project]](https://mapprior.github.io/)
- Sat2Graph: Road Graph Extraction through Graph-Tensor Encoding (ECCV 2020) [[Paper]](https://arxiv.org/pdf/2007.09547.pdf) [[Github]](https://github.com/songtaohe/Sat2Graph)
- Occ2Net: Robust Image Matching Based on 3D Occupancy Estimation for Occluded Regions (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2308.16160.pdf)
- QUEST: Query Stream for Vehicle-Infrastructure Cooperative Perception (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2308.01804.pdf)
- Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.15427.pdf)
- SyntheWorld: A Large-Scale Synthetic Dataset for Land Cover Mapping an  Building Change Detection (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.01907.pdf)
- Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.05731.pdf)
- BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2401.01065.pdf)
- BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.02136.pdf)
- Towards Vehicle-to-everything Autonomous Driving: A Survey on Collaborative Perception (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2308.16714.pdf)
- PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.04501.pdf)
- BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2309.02185.pdf) [[Github]](https://github.com/xmm-prio/BEVTrack)
- BEV-CV: Birds-Eye-View Transform for Cross-View Geo-Localisation (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.15363.pdf)
- UC-NERF: NEURAL RADIANCE FIELD FOR UNDER-CALIBRATED MULTI-VIEW CAMERAS IN AUTONOMOUS DRIVING (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2311.16945.pdf) [[Project]](https://kcheng1021.github.io/ucnerf.github.io/) [[Github]](https://github.com/kcheng1021/UC-NeRF)
- All for One, and One for All: UrbanSyn Dataset, the third Musketeer of Synthetic Driving Scenes (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.12176.pdf)
- BEVSeg2TP: Surround View Camera Bird’s-Eye-View Based Joint Vehicle Segmentation and Ego Vehicle Trajectory Prediction (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.13081.pdf)
- BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2308.01661.pdf)
- EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI (Arxiv 2023) [[Paper]](https://arxiv.org/pdf/2312.16170.pdf) [[Github]](https://github.com/OpenRobotLab/EmbodiedScan)
- A Vision-Centric Approach for Static Map Element Annotation (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2309.11754.pdf)
- C-BEV: Contrastive Bird’s Eye View Training for Cross-View Image Retrieval and 3-DoF Pose Estimation (Arxiv 2023) [[paper]](https://arxiv.org/pdf/2312.08060.pdf)
- Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.11499.pdf)
- GeoDecoder: Empowering Multimodal Map Understanding (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2401.15118.pdf)
- Fisheye Camera and Ultrasonic Sensor Fusion For Near-Field Obstacle Perception in Bird’s-Eye-View (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2402.00637.pdf)
- Text2Street: Controllable Text-to-image Generation for Street Views (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2402.04504.pdf)
- Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2402.13848)
- EV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.06600)
- OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation (Arxiv 2024) [[paper]](https://arxiv.org/abs/2403.11796)
- Bosch Street Dataset: A Multi-Modal Dataset with Imaging Radar for Automated Driving (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2407.12803)
- Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.13759) [[Github]](https://boyangdeng.com/streetscapes/)
- M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2403.12552)
- MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2405.01413)
- Window-to-Window BEV Representation Learning for Limited FoV Cross-View Geo-localization (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.06861)
- MapLocNet: Coarse-to-Fine Feature Registration for Visual Re-Localization in Navigation Maps (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2407.08561)
- Neural Semantic Map-Learning for Autonomous Vehicles (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2410.07780)
- AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction [[Paper]](Arxiv 2024) [[paper]](https://arxiv.org/pdf/2407.02598) [[Project]](https://autosplat.github.io/)
- MVPbev: Multi-view Perspective Image Generation from BEV with Test-time Controllability and Generalizability (Arxiv 2024) [[paper]](https://arxiv.org/abs/2407.19468) [[Github]](https://github.com/kkaiwwana/MVPbev)
- SkyDiffusion: Street-to-Satellite Image Synthesis with Diffusion Models and BEV Paradigm (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.01812) [[Github]](https://opendatalab.github.io/skydiffusion/)
- UrbanWorld: An Urban World Model for 3D City Generation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2407.11965)
- From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model (ICRA 2024) [[Paper]](https://arxiv.org/abs/2409.01014)
- Learning Content-Aware Multi-Modal Joint Input Pruning via Bird's-Eye-View Representation (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2410.07268)
- DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2409.05463) [[Project]](https://metadrivescape.github.io/papers_project/drivescapev1/index.html)
- BEVal: A Cross-dataset Evaluation Study of BEV Segmentation Models for Autononomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2408.16322)
- Bench2Drive-R: Turning Real World Data into Reactive Closed-Loop Autonomous Driving Benchmark by Generative Model (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2412.09647)
- RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2412.10734)
- OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous Driving (Arxiv 2024) [[paper]](https://arxiv.org/pdf/2412.10734)
- Hidden Biases of End-to-End Driving Datasets (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.09602) [[Github]](https://github.com/autonomousvision/carla_garage)
- Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2411.13626)
- VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2412.15544) [[Project]](https://www.huang-zilin.com/VLM-RL-website/) [[Github]](https://github.com/zihaosheng/VLM-RL)
- OLiDM: Object-aware LiDAR Diffusion Models for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/pdf/2412.17226) [[Project]](https://yanty123.github.io/OLiDM/) [[Github]](https://github.com/yanty123/OLiDM)
- DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes (Arxiv 2024) [[paper]](https://arxiv.org/abs/2412.19458) [[Project]](https://yvanliang.github.io/DriveEditor/) [[Github]](https://github.com/yvanliang/DriveEditor)
- HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving (Arxiv 2024) [[Paper]](https://arxiv.org/abs/2412.01407)
- Joint Perception and Prediction for Autonomous Driving: A Survey (Arxiv 2024) [[paper]](Joint Perception and Prediction for Autonomous Driving: A Survey)
- 3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic Segmentation in Autonomous Driving (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2501.14605)
- Range and Bird’s Eye View Fused Cross-Modal Visual Place Recognition (Arxiv 2025) [[paper]](https://arxiv.org/pdf/2502.11742)
- Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction (Arxiv 2025) [[Paper]](https://arxiv.org/pdf/2503.11091)
- BEVDiffLoc: End-to-End LiDAR Global Localization in BEV View based on Diffusion Model (Arxiv 2025) [[Paper]](https://arxiv.org/abs/2503.11372) [[Github]](https://github.com/nubot-nudt/BEVDiffLoc)
- RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation (Arxiv 2025) [[paper]](https://arxiv.org/abs/2502.14792)
